{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DesignOptimizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "38bd9a76f83f4d0b8d8a62d9cd526c93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5ee54d04dee840d1b3addd8609d0083d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_229c18af8b104884b59dafb81a1c6db5",
              "IPY_MODEL_85b7a96f345f4892b1f25f61c07a33dc"
            ]
          }
        },
        "5ee54d04dee840d1b3addd8609d0083d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "229c18af8b104884b59dafb81a1c6db5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_26249bdb2af845e18b3d81d3f2c4eaa3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c662d96a379c463ea32e81cd595d3aad"
          }
        },
        "85b7a96f345f4892b1f25f61c07a33dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_16bcbc72b3234a829f5119c2acdfe958",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:20&lt;00:00, 31727828.42it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d98728c067ac4a80a6e2701f8c1b3e8c"
          }
        },
        "26249bdb2af845e18b3d81d3f2c4eaa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c662d96a379c463ea32e81cd595d3aad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "16bcbc72b3234a829f5119c2acdfe958": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d98728c067ac4a80a6e2701f8c1b3e8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1T_gGTh_hSS"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWmz9744FOkp",
        "outputId": "7f662e35-d407-41f9-86bd-f2dcf50da422"
      },
      "source": [
        "%history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "import torch\n",
            "import numpy as np\n",
            "import torch.nn as nn\n",
            "import math\n",
            "from torch.optim.optimizer import Optimizer\n",
            "from google.colab import files\n",
            "version_higher = ( torch.__version__ >= \"1.5.0\" )\n",
            "\n",
            "class NestAdam(Optimizer):\n",
            "    def __init__(self, params, lr=1e-03, betas=(0.9, 0.999), eps=1e-16,\n",
            "                 weight_decay=0, amsgrad=False, weight_decouple=True, fixed_decay=False,\n",
            "                 rectify=True, degenerated_to_sgd=True):\n",
            "        if not 0.0 <= lr:\n",
            "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
            "        if not 0.0 <= eps:\n",
            "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
            "        if not 0.0 <= betas[0] < 1.0:\n",
            "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
            "        if not 0.0 <= betas[1] < 1.0:\n",
            "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
            "        if not 0.0 <= weight_decay:\n",
            "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
            "        \n",
            "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
            "            for param in params:\n",
            "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
            "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
            "\n",
            "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
            "                        weight_decay=weight_decay, amsgrad=amsgrad, buffer=[[None, None, None] for _ in range(10)])\n",
            "        super(NestAdam, self).__init__(params, defaults)\n",
            "\n",
            "        self.degenerated_to_sgd = degenerated_to_sgd\n",
            "        self.weight_decouple = weight_decouple\n",
            "        self.rectify = rectify\n",
            "        self.fixed_decay = fixed_decay\n",
            "\n",
            "    def __setstate__(self, state):\n",
            "        super(NestAdam, self).__setstate__(state)\n",
            "        for group in self.param_groups:\n",
            "            group.setdefault('amsgrad', False)\n",
            "\n",
            "    def step(self, closure=None):\n",
            "        \"\"\"Performs a single optimization step.\n",
            "        Arguments:\n",
            "            closure (callable, optional): A closure that reevaluates the model\n",
            "                and returns the loss.\n",
            "        \"\"\"\n",
            "        loss = None\n",
            "        if closure is not None:\n",
            "            loss = closure()\n",
            "\n",
            "        for group in self.param_groups:\n",
            "            for p in group['params']:\n",
            "                if p.grad is None:\n",
            "                    continue\n",
            "                grad = p.grad.data\n",
            "                if grad.is_sparse:\n",
            "                    raise RuntimeError(\n",
            "                        'NestAdam does not support sparse gradients, please consider SparseAdam instead')\n",
            "                amsgrad = group['amsgrad']\n",
            "\n",
            "                state = self.state[p]\n",
            "\n",
            "                beta1, beta2 = group['betas']\n",
            "\n",
            "                # State initialization\n",
            "                if len(state) == 0:\n",
            "                    state['step'] = 0\n",
            "                    # Exponential moving average of gradient values\n",
            "                    state['exp_avg'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    # Exponential moving average of squared gradient values\n",
            "                    state['exp_avg_var'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    if amsgrad:\n",
            "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
            "                        state['max_exp_avg_var'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                            if version_higher else torch.zeros_like(p.data)\n",
            "\n",
            "                # get current state variable\n",
            "                exp_avg, exp_avg_var = state['exp_avg'], state['exp_avg_var']\n",
            "\n",
            "                state['step'] += 1\n",
            "                bias_correction1 = 1 - beta1 ** state['step']\n",
            "                bias_correction2 = 1 - beta2 ** state['step']\n",
            "\n",
            "                # Approximate Nesterov's Accelerated Gradient\n",
            "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
            "                grad.mul_(1 - beta1).add_(exp_avg, alpha=beta1)\n",
            "\n",
            "                # Update first and second moment running average\n",
            "                # grad_residual = grad - exp_avg\n",
            "                # exp_avg_var.mul_(beta2).addcmul_(grad_residual, grad_residual, value=1 - beta2)\n",
            "                exp_avg_var.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
            "\n",
            "                if amsgrad:\n",
            "                    max_exp_avg_var = state['max_exp_avg_var']\n",
            "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
            "                    torch.max(max_exp_avg_var, exp_avg_var, out=max_exp_avg_var)\n",
            "\n",
            "                    # Use the max. for normalizing running avg. of gradient\n",
            "                    denom = (max_exp_avg_var.add_(group['eps']).sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
            "                else:\n",
            "                    denom = (exp_avg_var.add_(group['eps']).sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
            "\n",
            "                # perform weight decay, check if decoupled weight decay\n",
            "                if self.weight_decouple:\n",
            "                    if not self.fixed_decay:\n",
            "                        p.data.mul_(1.0 - group['lr'] * group['weight_decay'])\n",
            "                    else:\n",
            "                        p.data.mul_(1.0 - group['weight_decay'])\n",
            "                else:\n",
            "                    if group['weight_decay'] != 0:\n",
            "                        grad.add_(p.data, alpha=group['weight_decay'])\n",
            "\n",
            "                # update\n",
            "                if not self.rectify:\n",
            "                    # Default update\n",
            "                    step_size = group['lr'] / bias_correction1\n",
            "                    # p.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
            "                    p.data.addcdiv_(grad, denom, value=-step_size)\n",
            "\n",
            "                else:  # Rectified update, forked from RAdam\n",
            "                    buffered = group['buffer'][int(state['step'] % 10)]\n",
            "                    if state['step'] == buffered[0]:\n",
            "                        N_sma, step_size = buffered[1], buffered[2]\n",
            "                    else:\n",
            "                        buffered[0] = state['step']\n",
            "                        beta2_t = beta2 ** state['step']\n",
            "                        N_sma_max = 2 / (1 - beta2) - 1\n",
            "                        N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
            "                        buffered[1] = N_sma\n",
            "\n",
            "                        # more conservative since it's an approximated value\n",
            "                        if N_sma >= 5:\n",
            "                            step_size = math.sqrt(\n",
            "                                (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (\n",
            "                                        N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
            "                        elif self.degenerated_to_sgd:\n",
            "                            step_size = 1.0 / (1 - beta1 ** state['step'])\n",
            "                        else:\n",
            "                            step_size = -1\n",
            "                        buffered[2] = step_size\n",
            "\n",
            "                    if N_sma >= 5:\n",
            "                        denom = exp_avg_var.sqrt().add_(group['eps'])\n",
            "                        # p.data.addcdiv_(exp_avg, denom, value=-step_size * group['lr'])\n",
            "                        p.data.addcdiv_(grad, denom, value=-step_size * group['lr'])\n",
            "                    elif step_size > 0:\n",
            "                        # p.data.add_(exp_avg, alpha=-step_size * group['lr'])\n",
            "                        p.data.add_(grad, alpha=-step_size * group['lr'])\n",
            "\n",
            "        return loss\n",
            "import torchvision\n",
            "import torchvision.transforms as transforms\n",
            "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
            "train_transform = transforms.Compose([transforms.Resize(256),\n",
            "                                      transforms.RandomResizedCrop(224), \n",
            "                                      transforms.RandomHorizontalFlip(), \n",
            "                                      transforms.ToTensor(),\n",
            "                                      normalize])\n",
            "test_transform = transforms.Compose([transforms.Resize(256),\n",
            "                                     transforms.CenterCrop(224),\n",
            "                                     transforms.ToTensor(),\n",
            "                                     normalize])\n",
            "batch_size = 100\n",
            "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
            "                                        download=True, transform=train_transform)\n",
            "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
            "                                          shuffle=True, num_workers=2)\n",
            "\n",
            "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
            "                                       download=True, transform=test_transform)\n",
            "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
            "                                         shuffle=False, num_workers=2)\n",
            "import time\n",
            "\n",
            "def timeSince(since):\n",
            "    now = time.time()\n",
            "    s = now - since\n",
            "    m = s // 60\n",
            "    s -= m * 60\n",
            "    return '%dm %ds' % (m, s)\n",
            "import torchvision.models as models\n",
            "def evaluateImageModel(model, data, criterion, device):\n",
            "    model.eval()\n",
            "    total_loss = 0.0\n",
            "    correct = 0\n",
            "    total = 0\n",
            "    with torch.no_grad():\n",
            "        for image, labels in data:\n",
            "            image, labels = image.to(device), labels.to(device)\n",
            "            output = model(image)\n",
            "            pred = torch.argmax(output, dim=1)\n",
            "            correct += (pred == labels).sum().item()\n",
            "            total += len(labels)\n",
            "            loss = criterion(output, labels)\n",
            "            total_loss += loss.item()\n",
            "\n",
            "    return total_loss / len(data), 1 - correct / total\n",
            "\n",
            "def trainImageModel(model, train_data, val_data, n_epochs, criterion, optimizer, device, path):\n",
            "    model.train()\n",
            "    train_error_list = []\n",
            "    val_error_list = []\n",
            "    min_error = None\n",
            "    step = 0\n",
            "    print_every = len(train_data)\n",
            "    start = time.time()\n",
            "    for epoch in range(n_epochs):\n",
            "        running_loss = 0.0\n",
            "        running_correct = 0\n",
            "        running_total = 0\n",
            "        for image, labels in train_data:\n",
            "            optimizer.zero_grad()\n",
            "            step += 1\n",
            "            image, labels = image.to(device), labels.to(device)\n",
            "            output = model(image)\n",
            "            pred = torch.argmax(output, dim=1)\n",
            "            running_correct += (pred == labels).sum().item()\n",
            "            running_total += len(labels)\n",
            "            loss = criterion(output, labels)\n",
            "            running_loss += loss.item()\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "            if step % print_every == 0:\n",
            "                val_loss, val_error = evaluateImageModel(model, val_data, criterion, device)\n",
            "                print(('%d/%d (%s) train loss: %.3f, train error: %.2f%%, val loss: %.3f, val error: %.2f%%') %\n",
            "                      (epoch + 1, n_epochs, timeSince(start), running_loss / len(train_data), \n",
            "                       100 *(1 - running_correct / running_total), val_loss, 100 * val_error))\n",
            "                train_error_list.append(1 - running_correct / running_total)\n",
            "                val_error_list.append(val_error)\n",
            "                if min_error is None or min_error > val_error:\n",
            "                    if min_error is None:\n",
            "                        print(('Validation error rate in first epoch: %.2f%%') % (100 * val_error))\n",
            "                    else:\n",
            "                        print(('Validation error rate is decreasing: %.2f%% --> %.2f%%') % \n",
            "                              (100 * min_error, 100 * val_error))\n",
            "                    min_error = val_error\n",
            "                    print('Saving model...')\n",
            "                    torch.save(model, path)\n",
            "                \n",
            "                model.train()\n",
            "                running_loss = 0.0\n",
            "                running_correct = 0\n",
            "                running_total = 0\n",
            "    \n",
            "    return train_error_list, val_error_list\n",
            "model = models.vgg11()\n",
            "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
            "criterion = nn.CrossEntropyLoss()\n",
            "n_epochs = 50\n",
            "nadam_Res_train_list = None\n",
            "nadam_Res_test_list = None\n",
            "optimizer = NestAdam(model.parameters())\n",
            "model.to(device)\n",
            "path = 'nadamvgg.pt'\n",
            "nadam_Res_train_list, nadam_Res_test_list = trainImageModel(model, trainloader, testloader,\n",
            "                                                                    n_epochs, criterion, optimizer,\n",
            "                                                                    device, path)\n",
            "torch.save(nadam_Res_train_list, 'nadamvgg_train.pt')\n",
            "torch.save(nadam_Res_test_list, 'nadamvgg_test.pt')\n",
            "version_higher = ( torch.__version__ >= \"1.5.0\" )\n",
            "\n",
            "class NestAdam(Optimizer):\n",
            "    def __init__(self, params, lr=1e-03, betas=(0.9, 0.999), eps=1e-16,\n",
            "                 weight_decay=0, amsgrad=False, weight_decouple=True, fixed_decay=False,\n",
            "                 rectify=True, degenerated_to_sgd=True):\n",
            "        if not 0.0 <= lr:\n",
            "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
            "        if not 0.0 <= eps:\n",
            "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
            "        if not 0.0 <= betas[0] < 1.0:\n",
            "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
            "        if not 0.0 <= betas[1] < 1.0:\n",
            "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
            "        if not 0.0 <= weight_decay:\n",
            "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
            "        \n",
            "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
            "            for param in params:\n",
            "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
            "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
            "\n",
            "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
            "                        weight_decay=weight_decay, amsgrad=amsgrad, buffer=[[None, None, None] for _ in range(10)])\n",
            "        super(NestAdam, self).__init__(params, defaults)\n",
            "\n",
            "        self.degenerated_to_sgd = degenerated_to_sgd\n",
            "        self.weight_decouple = weight_decouple\n",
            "        self.rectify = rectify\n",
            "        self.fixed_decay = fixed_decay\n",
            "\n",
            "    def __setstate__(self, state):\n",
            "        super(NestAdam, self).__setstate__(state)\n",
            "        for group in self.param_groups:\n",
            "            group.setdefault('amsgrad', False)\n",
            "\n",
            "    def step(self, closure=None):\n",
            "        \"\"\"Performs a single optimization step.\n",
            "        Arguments:\n",
            "            closure (callable, optional): A closure that reevaluates the model\n",
            "                and returns the loss.\n",
            "        \"\"\"\n",
            "        loss = None\n",
            "        if closure is not None:\n",
            "            loss = closure()\n",
            "\n",
            "        for group in self.param_groups:\n",
            "            for p in group['params']:\n",
            "                if p.grad is None:\n",
            "                    continue\n",
            "                grad = p.grad.data\n",
            "                if grad.is_sparse:\n",
            "                    raise RuntimeError(\n",
            "                        'NestAdam does not support sparse gradients, please consider SparseAdam instead')\n",
            "                amsgrad = group['amsgrad']\n",
            "\n",
            "                state = self.state[p]\n",
            "\n",
            "                beta1, beta2 = group['betas']\n",
            "\n",
            "                # State initialization\n",
            "                if len(state) == 0:\n",
            "                    state['step'] = 0\n",
            "                    # Exponential moving average of gradient values\n",
            "                    state['exp_avg'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    # Exponential moving average of squared gradient values\n",
            "                    state['exp_avg_var'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    if amsgrad:\n",
            "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
            "                        state['max_exp_avg_var'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                            if version_higher else torch.zeros_like(p.data)\n",
            "\n",
            "                # get current state variable\n",
            "                exp_avg, exp_avg_var = state['exp_avg'], state['exp_avg_var']\n",
            "\n",
            "                state['step'] += 1\n",
            "                bias_correction1 = 1 - beta1 ** state['step']\n",
            "                bias_correction2 = 1 - beta2 ** state['step']\n",
            "\n",
            "                # Approximate Nesterov's Accelerated Gradient\n",
            "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
            "                grad_residual = grad - exp_avg\n",
            "                grad.mul_(1 - beta1).add_(exp_avg, alpha=beta1)\n",
            "\n",
            "                # Update first and second moment running average\n",
            "                # grad_residual = grad - exp_avg\n",
            "                exp_avg_var.mul_(beta2).addcmul_(grad_residual, grad_residual, value=1 - beta2)\n",
            "\n",
            "                if amsgrad:\n",
            "                    max_exp_avg_var = state['max_exp_avg_var']\n",
            "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
            "                    torch.max(max_exp_avg_var, exp_avg_var, out=max_exp_avg_var)\n",
            "\n",
            "                    # Use the max. for normalizing running avg. of gradient\n",
            "                    denom = (max_exp_avg_var.add_(group['eps']).sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
            "                else:\n",
            "                    denom = (exp_avg_var.add_(group['eps']).sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
            "\n",
            "                # perform weight decay, check if decoupled weight decay\n",
            "                if self.weight_decouple:\n",
            "                    if not self.fixed_decay:\n",
            "                        p.data.mul_(1.0 - group['lr'] * group['weight_decay'])\n",
            "                    else:\n",
            "                        p.data.mul_(1.0 - group['weight_decay'])\n",
            "                else:\n",
            "                    if group['weight_decay'] != 0:\n",
            "                        grad.add_(p.data, alpha=group['weight_decay'])\n",
            "\n",
            "                # update\n",
            "                if not self.rectify:\n",
            "                    # Default update\n",
            "                    step_size = group['lr'] / bias_correction1\n",
            "                    # p.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
            "                    p.data.addcdiv_(grad, denom, value=-step_size)\n",
            "\n",
            "                else:  # Rectified update, forked from RAdam\n",
            "                    buffered = group['buffer'][int(state['step'] % 10)]\n",
            "                    if state['step'] == buffered[0]:\n",
            "                        N_sma, step_size = buffered[1], buffered[2]\n",
            "                    else:\n",
            "                        buffered[0] = state['step']\n",
            "                        beta2_t = beta2 ** state['step']\n",
            "                        N_sma_max = 2 / (1 - beta2) - 1\n",
            "                        N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
            "                        buffered[1] = N_sma\n",
            "\n",
            "                        # more conservative since it's an approximated value\n",
            "                        if N_sma >= 5:\n",
            "                            step_size = math.sqrt(\n",
            "                                (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (\n",
            "                                        N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
            "                        elif self.degenerated_to_sgd:\n",
            "                            step_size = 1.0 / (1 - beta1 ** state['step'])\n",
            "                        else:\n",
            "                            step_size = -1\n",
            "                        buffered[2] = step_size\n",
            "\n",
            "                    if N_sma >= 5:\n",
            "                        denom = exp_avg_var.sqrt().add_(group['eps'])\n",
            "                        # p.data.addcdiv_(exp_avg, denom, value=-step_size * group['lr'])\n",
            "                        p.data.addcdiv_(grad, denom, value=-step_size * group['lr'])\n",
            "                    elif step_size > 0:\n",
            "                        # p.data.add_(exp_avg, alpha=-step_size * group['lr'])\n",
            "                        p.data.add_(grad, alpha=-step_size * group['lr'])\n",
            "\n",
            "        return loss\n",
            "model = models.vgg11()\n",
            "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
            "criterion = nn.CrossEntropyLoss()\n",
            "n_epochs = 50\n",
            "nadam_Res_train_list = None\n",
            "nadam_Res_test_list = None\n",
            "optimizer = NestAdam(model.parameters())\n",
            "model.to(device)\n",
            "path = 'nadamvgg.pt'\n",
            "nadam_Res_train_list, nadam_Res_test_list = trainImageModel(model, trainloader, testloader,\n",
            "                                                                    n_epochs, criterion, optimizer,\n",
            "                                                                    device, path)\n",
            "torch.save(nadam_Res_train_list, 'nadamvgg_train.pt')\n",
            "torch.save(nadam_Res_test_list, 'nadamvgg_test.pt')\n",
            "model = models.resnet34()\n",
            "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
            "criterion = nn.CrossEntropyLoss()\n",
            "n_epochs = 50\n",
            "nadam_Res_train_list = None\n",
            "nadam_Res_test_list = None\n",
            "optimizer = NestAdam(model.parameters())\n",
            "model.to(device)\n",
            "path = 'nadamRes.pt'\n",
            "nadam_Res_train_list, nadam_Res_test_list = trainImageModel(model, trainloader, testloader,\n",
            "                                                                    n_epochs, criterion, optimizer,\n",
            "                                                                    device, path)\n",
            "torch.save(nadam_Res_train_list, 'nadamres_train.pt')\n",
            "torch.save(nadam_Res_test_list, 'nadamres_test.pt')\n",
            "version_higher = ( torch.__version__ >= \"1.5.0\" )\n",
            "\n",
            "class NestAdam(Optimizer):\n",
            "    def __init__(self, params, lr=1e-03, betas=(0.9, 0.999), eps=1e-16,\n",
            "                 weight_decay=0, amsgrad=False, weight_decouple=True, fixed_decay=False,\n",
            "                 rectify=True, degenerated_to_sgd=True):\n",
            "        if not 0.0 <= lr:\n",
            "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
            "        if not 0.0 <= eps:\n",
            "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
            "        if not 0.0 <= betas[0] < 1.0:\n",
            "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
            "        if not 0.0 <= betas[1] < 1.0:\n",
            "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
            "        if not 0.0 <= weight_decay:\n",
            "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
            "        \n",
            "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
            "            for param in params:\n",
            "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
            "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
            "\n",
            "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
            "                        weight_decay=weight_decay, amsgrad=amsgrad, buffer=[[None, None, None] for _ in range(10)])\n",
            "        super(NestAdam, self).__init__(params, defaults)\n",
            "\n",
            "        self.degenerated_to_sgd = degenerated_to_sgd\n",
            "        self.weight_decouple = weight_decouple\n",
            "        self.rectify = rectify\n",
            "        self.fixed_decay = fixed_decay\n",
            "\n",
            "    def __setstate__(self, state):\n",
            "        super(NestAdam, self).__setstate__(state)\n",
            "        for group in self.param_groups:\n",
            "            group.setdefault('amsgrad', False)\n",
            "\n",
            "    def step(self, closure=None):\n",
            "        \"\"\"Performs a single optimization step.\n",
            "        Arguments:\n",
            "            closure (callable, optional): A closure that reevaluates the model\n",
            "                and returns the loss.\n",
            "        \"\"\"\n",
            "        loss = None\n",
            "        if closure is not None:\n",
            "            loss = closure()\n",
            "\n",
            "        for group in self.param_groups:\n",
            "            for p in group['params']:\n",
            "                if p.grad is None:\n",
            "                    continue\n",
            "                grad = p.grad.data\n",
            "                if grad.is_sparse:\n",
            "                    raise RuntimeError(\n",
            "                        'NestAdam does not support sparse gradients, please consider SparseAdam instead')\n",
            "                amsgrad = group['amsgrad']\n",
            "\n",
            "                state = self.state[p]\n",
            "\n",
            "                beta1, beta2 = group['betas']\n",
            "\n",
            "                # State initialization\n",
            "                if len(state) == 0:\n",
            "                    state['step'] = 0\n",
            "                    # Exponential moving average of gradient values\n",
            "                    state['exp_avg'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    # Exponential moving average of squared gradient values\n",
            "                    state['exp_avg_var'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    if amsgrad:\n",
            "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
            "                        state['max_exp_avg_var'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                            if version_higher else torch.zeros_like(p.data)\n",
            "\n",
            "                # get current state variable\n",
            "                exp_avg, exp_avg_var = state['exp_avg'], state['exp_avg_var']\n",
            "\n",
            "                state['step'] += 1\n",
            "                bias_correction1 = 1 - beta1 ** state['step']\n",
            "                bias_correction2 = 1 - beta2 ** state['step']\n",
            "\n",
            "                # Approximate Nesterov's Accelerated Gradient\n",
            "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
            "                grad_residual = grad - exp_avg\n",
            "                grad.mul_(1 - beta1).add_(exp_avg, alpha=beta1)\n",
            "\n",
            "                # Update first and second moment running average\n",
            "                # grad_residual = grad - exp_avg\n",
            "                exp_avg_var.mul_(beta2).addcmul_(grad_residual, grad_residual, value=1 - beta2)\n",
            "\n",
            "                if amsgrad:\n",
            "                    max_exp_avg_var = state['max_exp_avg_var']\n",
            "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
            "                    torch.max(max_exp_avg_var, exp_avg_var, out=max_exp_avg_var)\n",
            "\n",
            "                    # Use the max. for normalizing running avg. of gradient\n",
            "                    denom = (max_exp_avg_var.add_(group['eps']).sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
            "                else:\n",
            "                    denom = (exp_avg_var.add_(group['eps']).sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
            "\n",
            "                # perform weight decay, check if decoupled weight decay\n",
            "                if self.weight_decouple:\n",
            "                    if not self.fixed_decay:\n",
            "                        p.data.mul_(1.0 - group['lr'] * group['weight_decay'])\n",
            "                    else:\n",
            "                        p.data.mul_(1.0 - group['weight_decay'])\n",
            "                else:\n",
            "                    if group['weight_decay'] != 0:\n",
            "                        grad.add_(p.data, alpha=group['weight_decay'])\n",
            "\n",
            "                \"\"\"\n",
            "                # update\n",
            "                if not self.rectify:\n",
            "                    # Default update\n",
            "                    step_size = group['lr'] / bias_correction1\n",
            "                    # p.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
            "                    p.data.addcdiv_(grad, denom, value=-step_size)\n",
            "\n",
            "                else:  # Rectified update, forked from RAdam\n",
            "                    buffered = group['buffer'][int(state['step'] % 10)]\n",
            "                    if state['step'] == buffered[0]:\n",
            "                        N_sma, step_size = buffered[1], buffered[2]\n",
            "                    else:\n",
            "                        buffered[0] = state['step']\n",
            "                        beta2_t = beta2 ** state['step']\n",
            "                        N_sma_max = 2 / (1 - beta2) - 1\n",
            "                        N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
            "                        buffered[1] = N_sma\n",
            "\n",
            "                        # more conservative since it's an approximated value\n",
            "                        if N_sma >= 5:\n",
            "                            step_size = math.sqrt(\n",
            "                                (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (\n",
            "                                        N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
            "                        elif self.degenerated_to_sgd:\n",
            "                            step_size = 1.0 / (1 - beta1 ** state['step'])\n",
            "                        else:\n",
            "                            step_size = -1\n",
            "                        buffered[2] = step_size\n",
            "\n",
            "                    if N_sma >= 5:\n",
            "                        denom = exp_avg_var.sqrt().add_(group['eps'])\n",
            "                        # p.data.addcdiv_(exp_avg, denom, value=-step_size * group['lr'])\n",
            "                        p.data.addcdiv_(grad, denom, value=-step_size * group['lr'])\n",
            "                    elif step_size > 0:\n",
            "                        # p.data.add_(exp_avg, alpha=-step_size * group['lr'])\n",
            "                        p.data.add_(grad, alpha=-step_size * group['lr'])\n",
            "                \"\"\"\n",
            "                \n",
            "                step_size = group['lr'] / bias_correction1\n",
            "                p.data.addcdiv_(grad, denom, value=-step_size)\n",
            "\n",
            "        return loss\n",
            "model = models.resnet34()\n",
            "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
            "criterion = nn.CrossEntropyLoss()\n",
            "n_epochs = 50\n",
            "nadam_Res_train_list = None\n",
            "nadam_Res_test_list = None\n",
            "optimizer = NestAdam(model.parameters())\n",
            "model.to(device)\n",
            "path = 'nadamRes.pt'\n",
            "nadam_Res_train_list, nadam_Res_test_list = trainImageModel(model, trainloader, testloader,\n",
            "                                                                    n_epochs, criterion, optimizer,\n",
            "                                                                    device, path)\n",
            "torch.save(nadam_Res_train_list, 'nadamres_train.pt')\n",
            "torch.save(nadam_Res_test_list, 'nadamres_test.pt')\n",
            "version_higher = ( torch.__version__ >= \"1.5.0\" )\n",
            "\n",
            "class NestAdam(Optimizer):\n",
            "    def __init__(self, params, lr=1e-03, betas=(0.9, 0.999), eps=1e-16,\n",
            "                 weight_decay=0, amsgrad=False, weight_decouple=True, fixed_decay=False,\n",
            "                 rectify=True, degenerated_to_sgd=True):\n",
            "        if not 0.0 <= lr:\n",
            "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
            "        if not 0.0 <= eps:\n",
            "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
            "        if not 0.0 <= betas[0] < 1.0:\n",
            "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
            "        if not 0.0 <= betas[1] < 1.0:\n",
            "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
            "        if not 0.0 <= weight_decay:\n",
            "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
            "        \n",
            "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
            "            for param in params:\n",
            "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
            "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
            "\n",
            "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
            "                        weight_decay=weight_decay, amsgrad=amsgrad, buffer=[[None, None, None] for _ in range(10)])\n",
            "        super(NestAdam, self).__init__(params, defaults)\n",
            "\n",
            "        self.degenerated_to_sgd = degenerated_to_sgd\n",
            "        self.weight_decouple = weight_decouple\n",
            "        self.rectify = rectify\n",
            "        self.fixed_decay = fixed_decay\n",
            "\n",
            "    def __setstate__(self, state):\n",
            "        super(NestAdam, self).__setstate__(state)\n",
            "        for group in self.param_groups:\n",
            "            group.setdefault('amsgrad', False)\n",
            "\n",
            "    def step(self, closure=None):\n",
            "        \"\"\"Performs a single optimization step.\n",
            "        Arguments:\n",
            "            closure (callable, optional): A closure that reevaluates the model\n",
            "                and returns the loss.\n",
            "        \"\"\"\n",
            "        loss = None\n",
            "        if closure is not None:\n",
            "            loss = closure()\n",
            "\n",
            "        for group in self.param_groups:\n",
            "            for p in group['params']:\n",
            "                if p.grad is None:\n",
            "                    continue\n",
            "                grad = p.grad.data\n",
            "                if grad.is_sparse:\n",
            "                    raise RuntimeError(\n",
            "                        'NestAdam does not support sparse gradients, please consider SparseAdam instead')\n",
            "                amsgrad = group['amsgrad']\n",
            "\n",
            "                state = self.state[p]\n",
            "\n",
            "                beta1, beta2 = group['betas']\n",
            "\n",
            "                # State initialization\n",
            "                if len(state) == 0:\n",
            "                    state['step'] = 0\n",
            "                    # Exponential moving average of gradient values\n",
            "                    state['exp_avg'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    # Exponential moving average of squared gradient values\n",
            "                    state['exp_avg_var'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    if amsgrad:\n",
            "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
            "                        state['max_exp_avg_var'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                            if version_higher else torch.zeros_like(p.data)\n",
            "\n",
            "                # get current state variable\n",
            "                exp_avg, exp_avg_var = state['exp_avg'], state['exp_avg_var']\n",
            "\n",
            "                state['step'] += 1\n",
            "                bias_correction1 = 1 - beta1 ** state['step']\n",
            "                bias_correction2 = 1 - beta2 ** state['step']\n",
            "\n",
            "                # Approximate Nesterov's Accelerated Gradient\n",
            "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
            "                grad_ = grad.clone()\n",
            "                grad.mul_(1 - beta1).add_(exp_avg, alpha=beta1)\n",
            "\n",
            "                # Update first and second moment running average\n",
            "                grad_residual = grad - grad_\n",
            "                exp_avg_var.mul_(beta2).addcmul_(grad_residual, grad_residual, value=1 - beta2)\n",
            "\n",
            "                if amsgrad:\n",
            "                    max_exp_avg_var = state['max_exp_avg_var']\n",
            "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
            "                    torch.max(max_exp_avg_var, exp_avg_var, out=max_exp_avg_var)\n",
            "\n",
            "                    # Use the max. for normalizing running avg. of gradient\n",
            "                    denom = (max_exp_avg_var.add_(group['eps']).sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
            "                else:\n",
            "                    denom = (exp_avg_var.add_(group['eps']).sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
            "\n",
            "                # perform weight decay, check if decoupled weight decay\n",
            "                if self.weight_decouple:\n",
            "                    if not self.fixed_decay:\n",
            "                        p.data.mul_(1.0 - group['lr'] * group['weight_decay'])\n",
            "                    else:\n",
            "                        p.data.mul_(1.0 - group['weight_decay'])\n",
            "                else:\n",
            "                    if group['weight_decay'] != 0:\n",
            "                        grad.add_(p.data, alpha=group['weight_decay'])\n",
            "\n",
            "                \"\"\"\n",
            "                # update\n",
            "                if not self.rectify:\n",
            "                    # Default update\n",
            "                    step_size = group['lr'] / bias_correction1\n",
            "                    # p.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
            "                    p.data.addcdiv_(grad, denom, value=-step_size)\n",
            "\n",
            "                else:  # Rectified update, forked from RAdam\n",
            "                    buffered = group['buffer'][int(state['step'] % 10)]\n",
            "                    if state['step'] == buffered[0]:\n",
            "                        N_sma, step_size = buffered[1], buffered[2]\n",
            "                    else:\n",
            "                        buffered[0] = state['step']\n",
            "                        beta2_t = beta2 ** state['step']\n",
            "                        N_sma_max = 2 / (1 - beta2) - 1\n",
            "                        N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
            "                        buffered[1] = N_sma\n",
            "\n",
            "                        # more conservative since it's an approximated value\n",
            "                        if N_sma >= 5:\n",
            "                            step_size = math.sqrt(\n",
            "                                (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (\n",
            "                                        N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
            "                        elif self.degenerated_to_sgd:\n",
            "                            step_size = 1.0 / (1 - beta1 ** state['step'])\n",
            "                        else:\n",
            "                            step_size = -1\n",
            "                        buffered[2] = step_size\n",
            "\n",
            "                    if N_sma >= 5:\n",
            "                        denom = exp_avg_var.sqrt().add_(group['eps'])\n",
            "                        # p.data.addcdiv_(exp_avg, denom, value=-step_size * group['lr'])\n",
            "                        p.data.addcdiv_(grad, denom, value=-step_size * group['lr'])\n",
            "                    elif step_size > 0:\n",
            "                        # p.data.add_(exp_avg, alpha=-step_size * group['lr'])\n",
            "                        p.data.add_(grad, alpha=-step_size * group['lr'])\n",
            "                \"\"\"\n",
            "\n",
            "                step_size = group['lr'] / bias_correction1\n",
            "                p.data.addcdiv_(grad, denom, value=-step_size)\n",
            "\n",
            "        return loss\n",
            "model = models.resnet34()\n",
            "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
            "criterion = nn.CrossEntropyLoss()\n",
            "n_epochs = 50\n",
            "nadam_Res_train_list = None\n",
            "nadam_Res_test_list = None\n",
            "optimizer = NestAdam(model.parameters())\n",
            "model.to(device)\n",
            "path = 'nadamRes.pt'\n",
            "nadam_Res_train_list, nadam_Res_test_list = trainImageModel(model, trainloader, testloader,\n",
            "                                                                    n_epochs, criterion, optimizer,\n",
            "                                                                    device, path)\n",
            "torch.save(nadam_Res_train_list, 'nadamres_train.pt')\n",
            "torch.save(nadam_Res_test_list, 'nadamres_test.pt')\n",
            "version_higher = ( torch.__version__ >= \"1.5.0\" )\n",
            "\n",
            "class NestAdam(Optimizer):\n",
            "    def __init__(self, params, lr=1e-03, betas=(0.9, 0.999), eps=1e-16,\n",
            "                 weight_decay=0, amsgrad=False, weight_decouple=True, fixed_decay=False,\n",
            "                 rectify=True, degenerated_to_sgd=True):\n",
            "        if not 0.0 <= lr:\n",
            "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
            "        if not 0.0 <= eps:\n",
            "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
            "        if not 0.0 <= betas[0] < 1.0:\n",
            "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
            "        if not 0.0 <= betas[1] < 1.0:\n",
            "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
            "        if not 0.0 <= weight_decay:\n",
            "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
            "        \n",
            "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
            "            for param in params:\n",
            "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
            "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
            "\n",
            "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
            "                        weight_decay=weight_decay, amsgrad=amsgrad, buffer=[[None, None, None] for _ in range(10)])\n",
            "        super(NestAdam, self).__init__(params, defaults)\n",
            "\n",
            "        self.degenerated_to_sgd = degenerated_to_sgd\n",
            "        self.weight_decouple = weight_decouple\n",
            "        self.rectify = rectify\n",
            "        self.fixed_decay = fixed_decay\n",
            "\n",
            "    def __setstate__(self, state):\n",
            "        super(NestAdam, self).__setstate__(state)\n",
            "        for group in self.param_groups:\n",
            "            group.setdefault('amsgrad', False)\n",
            "\n",
            "    def step(self, closure=None):\n",
            "        \"\"\"Performs a single optimization step.\n",
            "        Arguments:\n",
            "            closure (callable, optional): A closure that reevaluates the model\n",
            "                and returns the loss.\n",
            "        \"\"\"\n",
            "        loss = None\n",
            "        if closure is not None:\n",
            "            loss = closure()\n",
            "\n",
            "        for group in self.param_groups:\n",
            "            for p in group['params']:\n",
            "                if p.grad is None:\n",
            "                    continue\n",
            "                grad = p.grad.data\n",
            "                if grad.is_sparse:\n",
            "                    raise RuntimeError(\n",
            "                        'NestAdam does not support sparse gradients, please consider SparseAdam instead')\n",
            "                amsgrad = group['amsgrad']\n",
            "\n",
            "                state = self.state[p]\n",
            "\n",
            "                beta1, beta2 = group['betas']\n",
            "\n",
            "                # State initialization\n",
            "                if len(state) == 0:\n",
            "                    state['step'] = 0\n",
            "                    # Exponential moving average of gradient values\n",
            "                    state['exp_avg'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    # Exponential moving average of squared gradient values\n",
            "                    state['exp_avg_var'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    if amsgrad:\n",
            "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
            "                        state['max_exp_avg_var'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                            if version_higher else torch.zeros_like(p.data)\n",
            "\n",
            "                # get current state variable\n",
            "                exp_avg, exp_avg_var = state['exp_avg'], state['exp_avg_var']\n",
            "\n",
            "                state['step'] += 1\n",
            "                bias_correction1 = 1 - beta1 ** state['step']\n",
            "                bias_correction2 = 1 - beta2 ** state['step']\n",
            "\n",
            "                # Approximate Nesterov's Accelerated Gradient\n",
            "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
            "                grad_ = grad.clone()\n",
            "                grad.mul_(1 - beta1).add_(exp_avg, alpha=beta1)\n",
            "\n",
            "                # Update first and second moment running average\n",
            "                grad_residual = grad - grad_\n",
            "                exp_avg_var.mul_(beta2).addcmul_(grad_residual, grad_residual, value=1 - beta2)\n",
            "\n",
            "                if amsgrad:\n",
            "                    max_exp_avg_var = state['max_exp_avg_var']\n",
            "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
            "                    torch.max(max_exp_avg_var, exp_avg_var, out=max_exp_avg_var)\n",
            "\n",
            "                    # Use the max. for normalizing running avg. of gradient\n",
            "                    denom = (max_exp_avg_var.add_(group['eps']).sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
            "                else:\n",
            "                    denom = (exp_avg_var.add_(group['eps']).sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
            "\n",
            "                # perform weight decay, check if decoupled weight decay\n",
            "                if self.weight_decouple:\n",
            "                    if not self.fixed_decay:\n",
            "                        p.data.mul_(1.0 - group['lr'] * group['weight_decay'])\n",
            "                    else:\n",
            "                        p.data.mul_(1.0 - group['weight_decay'])\n",
            "                else:\n",
            "                    if group['weight_decay'] != 0:\n",
            "                        grad.add_(p.data, alpha=group['weight_decay'])\n",
            "\n",
            "                # update\n",
            "                if not self.rectify:\n",
            "                    # Default update\n",
            "                    step_size = group['lr'] / bias_correction1\n",
            "                    # p.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
            "                    p.data.addcdiv_(grad, denom, value=-step_size)\n",
            "\n",
            "                else:  # Rectified update, forked from RAdam\n",
            "                    buffered = group['buffer'][int(state['step'] % 10)]\n",
            "                    if state['step'] == buffered[0]:\n",
            "                        N_sma, step_size = buffered[1], buffered[2]\n",
            "                    else:\n",
            "                        buffered[0] = state['step']\n",
            "                        beta2_t = beta2 ** state['step']\n",
            "                        N_sma_max = 2 / (1 - beta2) - 1\n",
            "                        N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
            "                        buffered[1] = N_sma\n",
            "\n",
            "                        # more conservative since it's an approximated value\n",
            "                        if N_sma >= 5:\n",
            "                            step_size = math.sqrt(\n",
            "                                (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (\n",
            "                                        N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
            "                        elif self.degenerated_to_sgd:\n",
            "                            step_size = 1.0 / (1 - beta1 ** state['step'])\n",
            "                        else:\n",
            "                            step_size = -1\n",
            "                        buffered[2] = step_size\n",
            "\n",
            "                    if N_sma >= 5:\n",
            "                        denom = exp_avg_var.sqrt().add_(group['eps'])\n",
            "                        # p.data.addcdiv_(exp_avg, denom, value=-step_size * group['lr'])\n",
            "                        p.data.addcdiv_(grad, denom, value=-step_size * group['lr'])\n",
            "                    elif step_size > 0:\n",
            "                        # p.data.add_(exp_avg, alpha=-step_size * group['lr'])\n",
            "                        p.data.add_(grad, alpha=-step_size * group['lr'])\n",
            "\n",
            "\n",
            "                # step_size = group['lr'] / bias_correction1\n",
            "                # p.data.addcdiv_(grad, denom, value=-step_size)\n",
            "\n",
            "        return loss\n",
            "model = models.resnet34()\n",
            "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
            "criterion = nn.CrossEntropyLoss()\n",
            "n_epochs = 50\n",
            "nadam_Res_train_list = None\n",
            "nadam_Res_test_list = None\n",
            "optimizer = NestAdam(model.parameters())\n",
            "model.to(device)\n",
            "path = 'nadamRes.pt'\n",
            "nadam_Res_train_list, nadam_Res_test_list = trainImageModel(model, trainloader, testloader,\n",
            "                                                                    n_epochs, criterion, optimizer,\n",
            "                                                                    device, path)\n",
            "torch.save(nadam_Res_train_list, 'nadamres_train.pt')\n",
            "torch.save(nadam_Res_test_list, 'nadamres_test.pt')\n",
            "version_higher = ( torch.__version__ >= \"1.5.0\" )\n",
            "\n",
            "class NestAdam(Optimizer):\n",
            "    def __init__(self, params, lr=1e-03, betas=(0.9, 0.999), eps=1e-16,\n",
            "                 weight_decay=0, amsgrad=False, weight_decouple=True, fixed_decay=False):\n",
            "        if not 0.0 <= lr:\n",
            "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
            "        if not 0.0 <= eps:\n",
            "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
            "        if not 0.0 <= betas[0] < 1.0:\n",
            "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
            "        if not 0.0 <= betas[1] < 1.0:\n",
            "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
            "        if not 0.0 <= weight_decay:\n",
            "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
            "        \n",
            "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
            "            for param in params:\n",
            "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
            "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
            "\n",
            "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
            "                        weight_decay=weight_decay, amsgrad=amsgrad, buffer=[[None, None, None] for _ in range(10)])\n",
            "        super(NestAdam, self).__init__(params, defaults)\n",
            "\n",
            "        self.weight_decouple = weight_decouple\n",
            "        self.fixed_decay = fixed_decay\n",
            "\n",
            "    def __setstate__(self, state):\n",
            "        super(NestAdam, self).__setstate__(state)\n",
            "        for group in self.param_groups:\n",
            "            group.setdefault('amsgrad', False)\n",
            "\n",
            "    def step(self, closure=None):\n",
            "        \"\"\"Performs a single optimization step.\n",
            "        Arguments:\n",
            "            closure (callable, optional): A closure that reevaluates the model\n",
            "                and returns the loss.\n",
            "        \"\"\"\n",
            "        loss = None\n",
            "        if closure is not None:\n",
            "            loss = closure()\n",
            "\n",
            "        for group in self.param_groups:\n",
            "            for p in group['params']:\n",
            "                if p.grad is None:\n",
            "                    continue\n",
            "                grad = p.grad.data\n",
            "                if grad.is_sparse:\n",
            "                    raise RuntimeError(\n",
            "                        'NestAdam does not support sparse gradients, please consider SparseAdam instead')\n",
            "                amsgrad = group['amsgrad']\n",
            "\n",
            "                state = self.state[p]\n",
            "\n",
            "                beta1, beta2 = group['betas']\n",
            "\n",
            "                # State initialization\n",
            "                if len(state) == 0:\n",
            "                    state['step'] = 0\n",
            "                    # Exponential moving average of gradient values\n",
            "                    state['exp_avg'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    # Exponential moving average of squared gradient values\n",
            "                    state['exp_avg_var'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    if amsgrad:\n",
            "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
            "                        state['max_exp_avg_var'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                            if version_higher else torch.zeros_like(p.data)\n",
            "\n",
            "                # get current state variable\n",
            "                exp_avg, exp_avg_var = state['exp_avg'], state['exp_avg_var']\n",
            "\n",
            "                state['step'] += 1\n",
            "                bias_correction1 = 1 - beta1 ** state['step']\n",
            "                bias_correction2 = 1 - beta2 ** state['step']\n",
            "\n",
            "                # Approximate Nesterov's Accelerated Gradient\n",
            "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
            "                grad_residual = grad - exp_avg\n",
            "                exp_avg_var.mul_(beta2).addcmul_(grad_residual, grad_residual, value=1 - beta2)\n",
            "                grad.mul_(1 - beta1).add_(exp_avg, alpha=beta1)\n",
            "\n",
            "                if amsgrad:\n",
            "                    max_exp_avg_var = state['max_exp_avg_var']\n",
            "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
            "                    torch.max(max_exp_avg_var, exp_avg_var, out=max_exp_avg_var)\n",
            "\n",
            "                    # Use the max. for normalizing running avg. of gradient\n",
            "                    denom = (max_exp_avg_var.add_(group['eps']).sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
            "                else:\n",
            "                    denom = (exp_avg_var.add_(group['eps']).sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
            "\n",
            "                # perform weight decay, check if decoupled weight decay\n",
            "                if self.weight_decouple:\n",
            "                    if not self.fixed_decay:\n",
            "                        p.data.mul_(1.0 - group['lr'] * group['weight_decay'])\n",
            "                    else:\n",
            "                        p.data.mul_(1.0 - group['weight_decay'])\n",
            "                else:\n",
            "                    if group['weight_decay'] != 0:\n",
            "                        grad.add_(p.data, alpha=group['weight_decay'])\n",
            "\n",
            "                step_size = group['lr'] / bias_correction1\n",
            "                p.data.addcdiv_(grad, denom, value=-step_size)\n",
            "\n",
            "        return loss\n",
            "model = models.resnet34()\n",
            "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
            "criterion = nn.CrossEntropyLoss()\n",
            "n_epochs = 50\n",
            "nadam_Res_train_list = None\n",
            "nadam_Res_test_list = None\n",
            "optimizer = NestAdam(model.parameters())\n",
            "model.to(device)\n",
            "path = 'nadamRes.pt'\n",
            "nadam_Res_train_list, nadam_Res_test_list = trainImageModel(model, trainloader, testloader,\n",
            "                                                                    n_epochs, criterion, optimizer,\n",
            "                                                                    device, path)\n",
            "torch.save(nadam_Res_train_list, 'nadamres_train.pt')\n",
            "torch.save(nadam_Res_test_list, 'nadamres_test.pt')\n",
            "files.download('nadamres_train.pt')\n",
            "files.download('nadamres_test.pt')\n",
            "model = models.shufflenet_v2_x1_0()\n",
            "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
            "criterion = nn.CrossEntropyLoss()\n",
            "n_epochs = 50\n",
            "nadam_train_list = None\n",
            "nadam_test_list = None\n",
            "optimizer = NestAdam(model.parameters())\n",
            "model.to(device)\n",
            "path = 'nadamshffule.pt'\n",
            "nadam_train_list, nadam_test_list = trainImageModel(model, trainloader, testloader,\n",
            "                                                                    n_epochs, criterion, optimizer,\n",
            "                                                                    device, path)\n",
            "torch.save(nadam_train_list, 'nadamres_train.pt')\n",
            "torch.save(nadam_test_list, 'nadamres_test.pt')\n",
            "files.download('nadamres_train.pt')\n",
            "files.download('nadamres_test.pt')\n",
            "model = models.mobilenet_v2()\n",
            "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
            "criterion = nn.CrossEntropyLoss()\n",
            "n_epochs = 50\n",
            "nadam_train_list = None\n",
            "nadam_test_list = None\n",
            "optimizer = NestAdam(model.parameters())\n",
            "model.to(device)\n",
            "name = 'nadammobile'\n",
            "path = name + '.pt'\n",
            "nadam_train_list, nadam_test_list = trainImageModel(model, trainloader, testloader,\n",
            "                                                                    n_epochs, criterion, optimizer,\n",
            "                                                                    device, path)\n",
            "torch.save(nadam_train_list, name + '_train.pt')\n",
            "torch.save(nadam_test_list, name + '_test.pt')\n",
            "files.download(name + '_train.pt')\n",
            "files.download(name + '_test.pt')\n",
            "version_higher = ( torch.__version__ >= \"1.5.0\" )\n",
            "\n",
            "class NAdam(Optimizer):\n",
            "    def __init__(self, params, lr=1e-03, betas=(0.9, 0.999), eps=1e-16,\n",
            "                 weight_decay=0, amsgrad=False, weight_decouple=True, fixed_decay=False):\n",
            "        if not 0.0 <= lr:\n",
            "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
            "        if not 0.0 <= eps:\n",
            "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
            "        if not 0.0 <= betas[0] < 1.0:\n",
            "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
            "        if not 0.0 <= betas[1] < 1.0:\n",
            "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
            "        if not 0.0 <= weight_decay:\n",
            "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
            "        \n",
            "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
            "            for param in params:\n",
            "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
            "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
            "\n",
            "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
            "                        weight_decay=weight_decay, amsgrad=amsgrad, buffer=[[None, None, None] for _ in range(10)])\n",
            "        super(NAdam, self).__init__(params, defaults)\n",
            "\n",
            "        self.weight_decouple = weight_decouple\n",
            "        self.fixed_decay = fixed_decay\n",
            "\n",
            "    def __setstate__(self, state):\n",
            "        super(NAdam, self).__setstate__(state)\n",
            "        for group in self.param_groups:\n",
            "            group.setdefault('amsgrad', False)\n",
            "\n",
            "    def step(self, closure=None):\n",
            "        \"\"\"Performs a single optimization step.\n",
            "        Arguments:\n",
            "            closure (callable, optional): A closure that reevaluates the model\n",
            "                and returns the loss.\n",
            "        \"\"\"\n",
            "        loss = None\n",
            "        if closure is not None:\n",
            "            loss = closure()\n",
            "\n",
            "        for group in self.param_groups:\n",
            "            for p in group['params']:\n",
            "                if p.grad is None:\n",
            "                    continue\n",
            "                grad = p.grad.data\n",
            "                if grad.is_sparse:\n",
            "                    raise RuntimeError(\n",
            "                        'NAdam does not support sparse gradients, please consider SparseAdam instead')\n",
            "                amsgrad = group['amsgrad']\n",
            "\n",
            "                state = self.state[p]\n",
            "\n",
            "                beta1, beta2 = group['betas']\n",
            "\n",
            "                # State initialization\n",
            "                if len(state) == 0:\n",
            "                    state['step'] = 0\n",
            "                    # Exponential moving average of gradient values\n",
            "                    state['exp_avg'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    # Exponential moving average of squared gradient values\n",
            "                    state['exp_avg_var'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    \n",
            "                    # Last gradients\n",
            "                    state['last_grad'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    # Last weights\n",
            "                    state['last_weight'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    if amsgrad:\n",
            "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
            "                        state['max_exp_avg_var'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                            if version_higher else torch.zeros_like(p.data)\n",
            "\n",
            "                # get current state variable\n",
            "                last_grad, exp_avg_var, last_weight = state['last_grad'], state['exp_avg_var'], state['last_weight']\n",
            "\n",
            "                state['step'] += 1\n",
            "\n",
            "\n",
            "                # Approximate Nesterov's Accelerated Gradient\n",
            "                grad.add_(grad - last_grad, alpha=beta1)\n",
            "                last_grad = p.grad.data\n",
            "                exp_avg_var.mul_(beta2).addcmul_(grad, grad)\n",
            "\n",
            "                if amsgrad:\n",
            "                    max_exp_avg_var = state['max_exp_avg_var']\n",
            "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
            "                    torch.max(max_exp_avg_var, exp_avg_var, out=max_exp_avg_var)\n",
            "\n",
            "                    # Use the max. for normalizing running avg. of gradient\n",
            "                    denom = max_exp_avg_var.add_(group['eps']).sqrt().add_(group['eps'])\n",
            "                else:\n",
            "                    denom = exp_avg_var.add_(group['eps']).sqrt().add_(group['eps'])\n",
            "\n",
            "                # perform weight decay, check if decoupled weight decay\n",
            "                if self.weight_decouple:\n",
            "                    if not self.fixed_decay:\n",
            "                        p.data.mul_(1.0 - group['lr'] * group['weight_decay'])\n",
            "                    else:\n",
            "                        p.data.mul_(1.0 - group['weight_decay'])\n",
            "                else:\n",
            "                    if group['weight_decay'] != 0:\n",
            "                        grad.add_(p.data, alpha=group['weight_decay'])\n",
            "\n",
            "                # Update\n",
            "                p.data.add_(p.data - last_weight, alpha=beta1)\n",
            "                p.data.addcdiv_(grad, denom, value=-step_size)\n",
            "                last_weight = p.data\n",
            "\n",
            "        return loss\n",
            "model = models.resnet34()\n",
            "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
            "criterion = nn.CrossEntropyLoss()\n",
            "n_epochs = 50\n",
            "nadam_train_list = None\n",
            "nadam_test_list = None\n",
            "optimizer = NestAdam(model.parameters())\n",
            "model.to(device)\n",
            "path = 'nadamRes.pt'\n",
            "nadam_train_list, nadam_test_list = trainImageModel(model, trainloader, testloader,\n",
            "                                                                    n_epochs, criterion, optimizer,\n",
            "                                                                    device, path)\n",
            "torch.save(nadam_train_list, 'nadamres_train.pt')\n",
            "torch.save(nadam_test_list, 'nadamres_test.pt')\n",
            "version_higher = ( torch.__version__ >= \"1.5.0\" )\n",
            "\n",
            "class NAdam(Optimizer):\n",
            "    def __init__(self, params, lr=1e-03, betas=(0.9, 0.999), eps=1e-16,\n",
            "                 weight_decay=0, amsgrad=False, weight_decouple=True, fixed_decay=False):\n",
            "        if not 0.0 <= lr:\n",
            "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
            "        if not 0.0 <= eps:\n",
            "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
            "        if not 0.0 <= betas[0] < 1.0:\n",
            "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
            "        if not 0.0 <= betas[1] < 1.0:\n",
            "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
            "        if not 0.0 <= weight_decay:\n",
            "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
            "        \n",
            "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
            "            for param in params:\n",
            "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
            "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
            "\n",
            "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
            "                        weight_decay=weight_decay, amsgrad=amsgrad, buffer=[[None, None, None] for _ in range(10)])\n",
            "        super(NAdam, self).__init__(params, defaults)\n",
            "\n",
            "        self.weight_decouple = weight_decouple\n",
            "        self.fixed_decay = fixed_decay\n",
            "\n",
            "    def __setstate__(self, state):\n",
            "        super(NAdam, self).__setstate__(state)\n",
            "        for group in self.param_groups:\n",
            "            group.setdefault('amsgrad', False)\n",
            "\n",
            "    def step(self, closure=None):\n",
            "        \"\"\"Performs a single optimization step.\n",
            "        Arguments:\n",
            "            closure (callable, optional): A closure that reevaluates the model\n",
            "                and returns the loss.\n",
            "        \"\"\"\n",
            "        loss = None\n",
            "        if closure is not None:\n",
            "            loss = closure()\n",
            "\n",
            "        for group in self.param_groups:\n",
            "            for p in group['params']:\n",
            "                if p.grad is None:\n",
            "                    continue\n",
            "                grad = p.grad.data\n",
            "                if grad.is_sparse:\n",
            "                    raise RuntimeError(\n",
            "                        'NAdam does not support sparse gradients, please consider SparseAdam instead')\n",
            "                amsgrad = group['amsgrad']\n",
            "\n",
            "                state = self.state[p]\n",
            "\n",
            "                beta1, beta2 = group['betas']\n",
            "\n",
            "                # State initialization\n",
            "                if len(state) == 0:\n",
            "                    state['step'] = 0\n",
            "                    # Exponential moving average of gradient values\n",
            "                    state['exp_avg'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    # Exponential moving average of squared gradient values\n",
            "                    state['exp_avg_var'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    \n",
            "                    # Last gradients\n",
            "                    state['last_grad'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    # Last weights\n",
            "                    state['last_weight'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    if amsgrad:\n",
            "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
            "                        state['max_exp_avg_var'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                            if version_higher else torch.zeros_like(p.data)\n",
            "\n",
            "                # get current state variable\n",
            "                last_grad, exp_avg_var, last_weight = state['last_grad'], state['exp_avg_var'], state['last_weight']\n",
            "\n",
            "                state['step'] += 1\n",
            "\n",
            "\n",
            "                # Approximate Nesterov's Accelerated Gradient\n",
            "                grad.add_(grad - last_grad, alpha=beta1)\n",
            "                last_grad = p.grad.data\n",
            "                exp_avg_var.mul_(beta2).addcmul_(grad, grad)\n",
            "\n",
            "                if amsgrad:\n",
            "                    max_exp_avg_var = state['max_exp_avg_var']\n",
            "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
            "                    torch.max(max_exp_avg_var, exp_avg_var, out=max_exp_avg_var)\n",
            "\n",
            "                    # Use the max. for normalizing running avg. of gradient\n",
            "                    denom = max_exp_avg_var.add_(group['eps']).sqrt().add_(group['eps'])\n",
            "                else:\n",
            "                    denom = exp_avg_var.add_(group['eps']).sqrt().add_(group['eps'])\n",
            "\n",
            "                # perform weight decay, check if decoupled weight decay\n",
            "                if self.weight_decouple:\n",
            "                    if not self.fixed_decay:\n",
            "                        p.data.mul_(1.0 - group['lr'] * group['weight_decay'])\n",
            "                    else:\n",
            "                        p.data.mul_(1.0 - group['weight_decay'])\n",
            "                else:\n",
            "                    if group['weight_decay'] != 0:\n",
            "                        grad.add_(p.data, alpha=group['weight_decay'])\n",
            "\n",
            "                # Update\n",
            "                p.data.add_(p.data - last_weight, alpha=beta1)\n",
            "                p.data.addcdiv_(grad, denom, value=-group['lr'])\n",
            "                last_weight = p.data\n",
            "\n",
            "        return loss\n",
            "version_higher = ( torch.__version__ >= \"1.5.0\" )\n",
            "\n",
            "class NAdam(Optimizer):\n",
            "    def __init__(self, params, lr=1e-03, betas=(0.9, 0.999), eps=1e-16,\n",
            "                 weight_decay=0, amsgrad=False, weight_decouple=True, fixed_decay=False):\n",
            "        if not 0.0 <= lr:\n",
            "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
            "        if not 0.0 <= eps:\n",
            "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
            "        if not 0.0 <= betas[0] < 1.0:\n",
            "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
            "        if not 0.0 <= betas[1] < 1.0:\n",
            "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
            "        if not 0.0 <= weight_decay:\n",
            "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
            "        \n",
            "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
            "            for param in params:\n",
            "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
            "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
            "\n",
            "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
            "                        weight_decay=weight_decay, amsgrad=amsgrad, buffer=[[None, None, None] for _ in range(10)])\n",
            "        super(NAdam, self).__init__(params, defaults)\n",
            "\n",
            "        self.weight_decouple = weight_decouple\n",
            "        self.fixed_decay = fixed_decay\n",
            "\n",
            "    def __setstate__(self, state):\n",
            "        super(NAdam, self).__setstate__(state)\n",
            "        for group in self.param_groups:\n",
            "            group.setdefault('amsgrad', False)\n",
            "\n",
            "    def step(self, closure=None):\n",
            "        \"\"\"Performs a single optimization step.\n",
            "        Arguments:\n",
            "            closure (callable, optional): A closure that reevaluates the model\n",
            "                and returns the loss.\n",
            "        \"\"\"\n",
            "        loss = None\n",
            "        if closure is not None:\n",
            "            loss = closure()\n",
            "\n",
            "        for group in self.param_groups:\n",
            "            for p in group['params']:\n",
            "                if p.grad is None:\n",
            "                    continue\n",
            "                grad = p.grad.data\n",
            "                if grad.is_sparse:\n",
            "                    raise RuntimeError(\n",
            "                        'NAdam does not support sparse gradients, please consider SparseAdam instead')\n",
            "                amsgrad = group['amsgrad']\n",
            "\n",
            "                state = self.state[p]\n",
            "\n",
            "                beta1, beta2 = group['betas']\n",
            "\n",
            "                # State initialization\n",
            "                if len(state) == 0:\n",
            "                    state['step'] = 0\n",
            "                    # Exponential moving average of gradient values\n",
            "                    state['exp_avg'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    # Exponential moving average of squared gradient values\n",
            "                    state['exp_avg_var'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    \n",
            "                    # Last gradients\n",
            "                    state['last_grad'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    # Last weights\n",
            "                    state['last_weight'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    if amsgrad:\n",
            "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
            "                        state['max_exp_avg_var'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                            if version_higher else torch.zeros_like(p.data)\n",
            "\n",
            "                # get current state variable\n",
            "                last_grad, exp_avg_var, last_weight = state['last_grad'], state['exp_avg_var'], state['last_weight']\n",
            "\n",
            "                state['step'] += 1\n",
            "\n",
            "\n",
            "                # Approximate Nesterov's Accelerated Gradient\n",
            "                grad.add_(grad - last_grad, alpha=beta1)\n",
            "                last_grad = p.grad.data\n",
            "                exp_avg_var.mul_(beta2).addcmul_(grad, grad)\n",
            "\n",
            "                if amsgrad:\n",
            "                    max_exp_avg_var = state['max_exp_avg_var']\n",
            "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
            "                    torch.max(max_exp_avg_var, exp_avg_var, out=max_exp_avg_var)\n",
            "\n",
            "                    # Use the max. for normalizing running avg. of gradient\n",
            "                    denom = max_exp_avg_var.add_(group['eps']).sqrt().add_(group['eps'])\n",
            "                else:\n",
            "                    denom = exp_avg_var.add_(group['eps']).sqrt().add_(group['eps'])\n",
            "\n",
            "                # perform weight decay, check if decoupled weight decay\n",
            "                if self.weight_decouple:\n",
            "                    if not self.fixed_decay:\n",
            "                        p.data.mul_(1.0 - group['lr'] * group['weight_decay'])\n",
            "                    else:\n",
            "                        p.data.mul_(1.0 - group['weight_decay'])\n",
            "                else:\n",
            "                    if group['weight_decay'] != 0:\n",
            "                        grad.add_(p.data, alpha=group['weight_decay'])\n",
            "\n",
            "                # Update\n",
            "                p.data.add_(p.data - last_weight, alpha=beta1)\n",
            "                p.data.addcdiv_(grad, denom, value=-group['lr'])\n",
            "                last_weight = p.data\n",
            "\n",
            "        return loss\n",
            "model = models.resnet34()\n",
            "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
            "criterion = nn.CrossEntropyLoss()\n",
            "n_epochs = 50\n",
            "nadam_train_list = None\n",
            "nadam_test_list = None\n",
            "optimizer = NestAdam(model.parameters())\n",
            "model.to(device)\n",
            "path = 'nadamRes.pt'\n",
            "nadam_train_list, nadam_test_list = trainImageModel(model, trainloader, testloader,\n",
            "                                                                    n_epochs, criterion, optimizer,\n",
            "                                                                    device, path)\n",
            "torch.save(nadam_train_list, 'nadamres_train.pt')\n",
            "torch.save(nadam_test_list, 'nadamres_test.pt')\n",
            "version_higher = ( torch.__version__ >= \"1.5.0\" )\n",
            "\n",
            "class NAdam(Optimizer):\n",
            "    def __init__(self, params, lr=1e-03, betas=(0.9, 0.999), eps=1e-16,\n",
            "                 weight_decay=0, amsgrad=False, weight_decouple=True, fixed_decay=False):\n",
            "        if not 0.0 <= lr:\n",
            "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
            "        if not 0.0 <= eps:\n",
            "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
            "        if not 0.0 <= betas[0] < 1.0:\n",
            "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
            "        if not 0.0 <= betas[1] < 1.0:\n",
            "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
            "        if not 0.0 <= weight_decay:\n",
            "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
            "        \n",
            "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
            "            for param in params:\n",
            "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
            "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
            "\n",
            "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
            "                        weight_decay=weight_decay, amsgrad=amsgrad, buffer=[[None, None, None] for _ in range(10)])\n",
            "        super(NAdam, self).__init__(params, defaults)\n",
            "\n",
            "        self.weight_decouple = weight_decouple\n",
            "        self.fixed_decay = fixed_decay\n",
            "\n",
            "    def __setstate__(self, state):\n",
            "        super(NAdam, self).__setstate__(state)\n",
            "        for group in self.param_groups:\n",
            "            group.setdefault('amsgrad', False)\n",
            "\n",
            "    def step(self, closure=None):\n",
            "        \"\"\"Performs a single optimization step.\n",
            "        Arguments:\n",
            "            closure (callable, optional): A closure that reevaluates the model\n",
            "                and returns the loss.\n",
            "        \"\"\"\n",
            "        loss = None\n",
            "        if closure is not None:\n",
            "            loss = closure()\n",
            "\n",
            "        for group in self.param_groups:\n",
            "            for p in group['params']:\n",
            "                if p.grad is None:\n",
            "                    continue\n",
            "                grad = p.grad.data\n",
            "                if grad.is_sparse:\n",
            "                    raise RuntimeError(\n",
            "                        'NAdam does not support sparse gradients, please consider SparseAdam instead')\n",
            "                amsgrad = group['amsgrad']\n",
            "\n",
            "                state = self.state[p]\n",
            "\n",
            "                beta1, beta2 = group['betas']\n",
            "\n",
            "                # State initialization\n",
            "                if len(state) == 0:\n",
            "                    state['step'] = 0\n",
            "                    # Exponential moving average of gradient values\n",
            "                    state['exp_avg'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    # Exponential moving average of squared gradient values\n",
            "                    state['exp_avg_var'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    \n",
            "                    # Last gradients\n",
            "                    state['last_grad'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    # Last weights\n",
            "                    state['last_weight'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    if amsgrad:\n",
            "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
            "                        state['max_exp_avg_var'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                            if version_higher else torch.zeros_like(p.data)\n",
            "\n",
            "                # get current state variable\n",
            "                last_grad, exp_avg_var, last_weight = state['last_grad'], state['exp_avg_var'], state['last_weight']\n",
            "\n",
            "                state['step'] += 1\n",
            "\n",
            "\n",
            "                # Approximate Nesterov's Accelerated Gradient\n",
            "                grad.add_(grad - last_grad, alpha=beta1)\n",
            "                last_grad = p.grad.data\n",
            "                exp_avg_var.mul_(beta2).addcmul_(grad, grad)\n",
            "\n",
            "                if amsgrad:\n",
            "                    max_exp_avg_var = state['max_exp_avg_var']\n",
            "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
            "                    torch.max(max_exp_avg_var, exp_avg_var, out=max_exp_avg_var)\n",
            "\n",
            "                    # Use the max. for normalizing running avg. of gradient\n",
            "                    denom = max_exp_avg_var.sqrt().add_(group['eps'])\n",
            "                else:\n",
            "                    denom = exp_avg_var.add_(group['eps'])\n",
            "\n",
            "                # perform weight decay, check if decoupled weight decay\n",
            "                if self.weight_decouple:\n",
            "                    if not self.fixed_decay:\n",
            "                        p.data.mul_(1.0 - group['lr'] * group['weight_decay'])\n",
            "                    else:\n",
            "                        p.data.mul_(1.0 - group['weight_decay'])\n",
            "                else:\n",
            "                    if group['weight_decay'] != 0:\n",
            "                        grad.add_(p.data, alpha=group['weight_decay'])\n",
            "\n",
            "                # Update\n",
            "                p.data.add_(p.data - last_weight, alpha=beta1)\n",
            "                p.data.addcdiv_(grad, denom, value=-group['lr'])\n",
            "                last_weight = p.data\n",
            "\n",
            "        return loss\n",
            "model = models.resnet34()\n",
            "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
            "criterion = nn.CrossEntropyLoss()\n",
            "n_epochs = 50\n",
            "nadam_train_list = None\n",
            "nadam_test_list = None\n",
            "optimizer = NestAdam(model.parameters())\n",
            "model.to(device)\n",
            "path = 'nadamRes.pt'\n",
            "nadam_train_list, nadam_test_list = trainImageModel(model, trainloader, testloader,\n",
            "                                                                    n_epochs, criterion, optimizer,\n",
            "                                                                    device, path)\n",
            "torch.save(nadam_train_list, 'nadamres_train.pt')\n",
            "torch.save(nadam_test_list, 'nadamres_test.pt')\n",
            "version_higher = ( torch.__version__ >= \"1.5.0\" )\n",
            "\n",
            "class NAdam(Optimizer):\n",
            "    def __init__(self, params, lr=1e-03, betas=(0.9, 0.999), eps=1e-16,\n",
            "                 weight_decay=0, amsgrad=False, weight_decouple=True, fixed_decay=False):\n",
            "        if not 0.0 <= lr:\n",
            "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
            "        if not 0.0 <= eps:\n",
            "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
            "        if not 0.0 <= betas[0] < 1.0:\n",
            "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
            "        if not 0.0 <= betas[1] < 1.0:\n",
            "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
            "        if not 0.0 <= weight_decay:\n",
            "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
            "        \n",
            "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
            "            for param in params:\n",
            "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
            "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
            "\n",
            "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
            "                        weight_decay=weight_decay, amsgrad=amsgrad, buffer=[[None, None, None] for _ in range(10)])\n",
            "        super(NAdam, self).__init__(params, defaults)\n",
            "\n",
            "        self.weight_decouple = weight_decouple\n",
            "        self.fixed_decay = fixed_decay\n",
            "\n",
            "    def __setstate__(self, state):\n",
            "        super(NAdam, self).__setstate__(state)\n",
            "        for group in self.param_groups:\n",
            "            group.setdefault('amsgrad', False)\n",
            "\n",
            "    def step(self, closure=None):\n",
            "        \"\"\"Performs a single optimization step.\n",
            "        Arguments:\n",
            "            closure (callable, optional): A closure that reevaluates the model\n",
            "                and returns the loss.\n",
            "        \"\"\"\n",
            "        loss = None\n",
            "        if closure is not None:\n",
            "            loss = closure()\n",
            "\n",
            "        for group in self.param_groups:\n",
            "            for p in group['params']:\n",
            "                if p.grad is None:\n",
            "                    continue\n",
            "                grad = p.grad.data\n",
            "                if grad.is_sparse:\n",
            "                    raise RuntimeError(\n",
            "                        'NAdam does not support sparse gradients, please consider SparseAdam instead')\n",
            "                amsgrad = group['amsgrad']\n",
            "\n",
            "                state = self.state[p]\n",
            "\n",
            "                beta1, beta2 = group['betas']\n",
            "\n",
            "                # State initialization\n",
            "                if len(state) == 0:\n",
            "                    state['step'] = 0\n",
            "                    # Exponential moving average of gradient values\n",
            "                    state['exp_avg'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    # Exponential moving average of squared gradient values\n",
            "                    state['exp_avg_var'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    \n",
            "                    # Last gradients\n",
            "                    state['last_grad'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    # Last weights\n",
            "                    state['last_weight'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    if amsgrad:\n",
            "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
            "                        state['max_exp_avg_var'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                            if version_higher else torch.zeros_like(p.data)\n",
            "\n",
            "                # get current state variable\n",
            "                exp_avg, last_grad, exp_avg_var, last_weight = state['exp_avg'], state['last_grad'], state['exp_avg_var'], state['last_weight']\n",
            "\n",
            "                state['step'] += 1\n",
            "\n",
            "\n",
            "                # Approximate Nesterov's Accelerated Gradient\n",
            "                exp_avg.mul_(beta1).add_(grad)\n",
            "                grad.add_(grad - last_grad, alpha=beta1)\n",
            "                last_grad = p.grad.data\n",
            "                exp_avg_var.mul_(beta2).addcmul_(grad, grad)\n",
            "\n",
            "                if amsgrad:\n",
            "                    max_exp_avg_var = state['max_exp_avg_var']\n",
            "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
            "                    torch.max(max_exp_avg_var, exp_avg_var, out=max_exp_avg_var)\n",
            "\n",
            "                    # Use the max. for normalizing running avg. of gradient\n",
            "                    denom = max_exp_avg_var.sqrt().add_(group['eps'])\n",
            "                else:\n",
            "                    denom = exp_avg_var.add_(group['eps'])\n",
            "\n",
            "                # perform weight decay, check if decoupled weight decay\n",
            "                if self.weight_decouple:\n",
            "                    if not self.fixed_decay:\n",
            "                        p.data.mul_(1.0 - group['lr'] * group['weight_decay'])\n",
            "                    else:\n",
            "                        p.data.mul_(1.0 - group['weight_decay'])\n",
            "                else:\n",
            "                    if group['weight_decay'] != 0:\n",
            "                        grad.add_(p.data, alpha=group['weight_decay'])\n",
            "\n",
            "                # Update\n",
            "                p.data.add_(p.data - last_weight, alpha=beta1)\n",
            "                p.data.addcdiv_(exp_avg, denom, value=-group['lr'])\n",
            "                last_weight = p.data\n",
            "\n",
            "        return loss\n",
            "model = models.resnet34()\n",
            "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
            "criterion = nn.CrossEntropyLoss()\n",
            "n_epochs = 50\n",
            "nadam_train_list = None\n",
            "nadam_test_list = None\n",
            "optimizer = NAdam(model.parameters())\n",
            "model.to(device)\n",
            "path = 'nadamRes.pt'\n",
            "nadam_train_list, nadam_test_list = trainImageModel(model, trainloader, testloader,\n",
            "                                                                    n_epochs, criterion, optimizer,\n",
            "                                                                    device, path)\n",
            "torch.save(nadam_train_list, 'nadamres_train.pt')\n",
            "torch.save(nadam_test_list, 'nadamres_test.pt')\n",
            "version_higher = ( torch.__version__ >= \"1.5.0\" )\n",
            "\n",
            "class NAdam(Optimizer):\n",
            "    def __init__(self, params, lr=1e-03, betas=(0.9, 0.999), eps=1e-16,\n",
            "                 weight_decay=0, amsgrad=False, weight_decouple=True, fixed_decay=False):\n",
            "        if not 0.0 <= lr:\n",
            "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
            "        if not 0.0 <= eps:\n",
            "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
            "        if not 0.0 <= betas[0] < 1.0:\n",
            "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
            "        if not 0.0 <= betas[1] < 1.0:\n",
            "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
            "        if not 0.0 <= weight_decay:\n",
            "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
            "        \n",
            "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
            "            for param in params:\n",
            "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
            "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
            "\n",
            "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
            "                        weight_decay=weight_decay, amsgrad=amsgrad, buffer=[[None, None, None] for _ in range(10)])\n",
            "        super(NAdam, self).__init__(params, defaults)\n",
            "\n",
            "        self.weight_decouple = weight_decouple\n",
            "        self.fixed_decay = fixed_decay\n",
            "\n",
            "    def __setstate__(self, state):\n",
            "        super(NAdam, self).__setstate__(state)\n",
            "        for group in self.param_groups:\n",
            "            group.setdefault('amsgrad', False)\n",
            "\n",
            "    def step(self, closure=None):\n",
            "        \"\"\"Performs a single optimization step.\n",
            "        Arguments:\n",
            "            closure (callable, optional): A closure that reevaluates the model\n",
            "                and returns the loss.\n",
            "        \"\"\"\n",
            "        loss = None\n",
            "        if closure is not None:\n",
            "            loss = closure()\n",
            "\n",
            "        for group in self.param_groups:\n",
            "            for p in group['params']:\n",
            "                if p.grad is None:\n",
            "                    continue\n",
            "                grad = p.grad.data\n",
            "                if grad.is_sparse:\n",
            "                    raise RuntimeError(\n",
            "                        'NAdam does not support sparse gradients, please consider SparseAdam instead')\n",
            "                amsgrad = group['amsgrad']\n",
            "\n",
            "                state = self.state[p]\n",
            "\n",
            "                beta1, beta2 = group['betas']\n",
            "\n",
            "                # State initialization\n",
            "                if len(state) == 0:\n",
            "                    state['step'] = 0\n",
            "                    # Exponential moving average of gradient values\n",
            "                    state['exp_avg'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    # Exponential moving average of squared gradient values\n",
            "                    state['exp_avg_var'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    \n",
            "                    # Last gradients\n",
            "                    state['last_grad'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    # Last weights\n",
            "                    state['last_weight'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    if amsgrad:\n",
            "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
            "                        state['max_exp_avg_var'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                            if version_higher else torch.zeros_like(p.data)\n",
            "\n",
            "                # get current state variable\n",
            "                exp_avg, last_grad, exp_avg_var, last_weight = state['exp_avg'], state['last_grad'], state['exp_avg_var'], state['last_weight']\n",
            "\n",
            "                state['step'] += 1\n",
            "\n",
            "                # Approximate Nesterov's Accelerated Gradient\n",
            "                exp_avg.mul_(beta1).add_(grad)\n",
            "                grad.add_(grad - last_grad, alpha=beta1)\n",
            "                last_grad = p.grad.data.clone()\n",
            "                exp_avg_var.mul_(beta2).addcmul_(grad, grad)\n",
            "\n",
            "                if amsgrad:\n",
            "                    max_exp_avg_var = state['max_exp_avg_var']\n",
            "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
            "                    torch.max(max_exp_avg_var, exp_avg_var, out=max_exp_avg_var)\n",
            "\n",
            "                    # Use the max. for normalizing running avg. of gradient\n",
            "                    denom = max_exp_avg_var.sqrt().add_(group['eps'])\n",
            "                else:\n",
            "                    denom = exp_avg_var.add_(group['eps'])\n",
            "\n",
            "                # perform weight decay, check if decoupled weight decay\n",
            "                if self.weight_decouple:\n",
            "                    if not self.fixed_decay:\n",
            "                        p.data.mul_(1.0 - group['lr'] * group['weight_decay'])\n",
            "                    else:\n",
            "                        p.data.mul_(1.0 - group['weight_decay'])\n",
            "                else:\n",
            "                    if group['weight_decay'] != 0:\n",
            "                        grad.add_(p.data, alpha=group['weight_decay'])\n",
            "\n",
            "                # Update\n",
            "                p.data.add_(p.data - last_weight, alpha=beta1)\n",
            "                p.data.addcdiv_(exp_avg, denom, value=-group['lr'])\n",
            "                last_weight = p.data.clone()\n",
            "\n",
            "        return loss\n",
            "model = models.resnet34()\n",
            "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
            "criterion = nn.CrossEntropyLoss()\n",
            "n_epochs = 50\n",
            "nadam_train_list = None\n",
            "nadam_test_list = None\n",
            "optimizer = NAdam(model.parameters())\n",
            "model.to(device)\n",
            "path = 'nadamRes.pt'\n",
            "nadam_train_list, nadam_test_list = trainImageModel(model, trainloader, testloader,\n",
            "                                                                    n_epochs, criterion, optimizer,\n",
            "                                                                    device, path)\n",
            "torch.save(nadam_train_list, 'nadamres_train.pt')\n",
            "torch.save(nadam_test_list, 'nadamres_test.pt')\n",
            "version_higher = ( torch.__version__ >= \"1.5.0\" )\n",
            "\n",
            "class NAdam(Optimizer):\n",
            "    def __init__(self, params, lr=1e-03, betas=(0.9, 0.999), eps=1e-16,\n",
            "                 weight_decay=0, amsgrad=False, weight_decouple=True, fixed_decay=False):\n",
            "        if not 0.0 <= lr:\n",
            "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
            "        if not 0.0 <= eps:\n",
            "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
            "        if not 0.0 <= betas[0] < 1.0:\n",
            "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
            "        if not 0.0 <= betas[1] < 1.0:\n",
            "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
            "        if not 0.0 <= weight_decay:\n",
            "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
            "        \n",
            "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
            "            for param in params:\n",
            "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
            "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
            "\n",
            "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
            "                        weight_decay=weight_decay, amsgrad=amsgrad, buffer=[[None, None, None] for _ in range(10)])\n",
            "        super(NAdam, self).__init__(params, defaults)\n",
            "\n",
            "        self.weight_decouple = weight_decouple\n",
            "        self.fixed_decay = fixed_decay\n",
            "\n",
            "    def __setstate__(self, state):\n",
            "        super(NAdam, self).__setstate__(state)\n",
            "        for group in self.param_groups:\n",
            "            group.setdefault('amsgrad', False)\n",
            "\n",
            "    def step(self, closure=None):\n",
            "        \"\"\"Performs a single optimization step.\n",
            "        Arguments:\n",
            "            closure (callable, optional): A closure that reevaluates the model\n",
            "                and returns the loss.\n",
            "        \"\"\"\n",
            "        loss = None\n",
            "        if closure is not None:\n",
            "            loss = closure()\n",
            "\n",
            "        for group in self.param_groups:\n",
            "            for p in group['params']:\n",
            "                if p.grad is None:\n",
            "                    continue\n",
            "                grad = p.grad.data\n",
            "                if grad.is_sparse:\n",
            "                    raise RuntimeError(\n",
            "                        'NAdam does not support sparse gradients, please consider SparseAdam instead')\n",
            "                amsgrad = group['amsgrad']\n",
            "\n",
            "                state = self.state[p]\n",
            "\n",
            "                beta1, beta2 = group['betas']\n",
            "\n",
            "                # State initialization\n",
            "                if len(state) == 0:\n",
            "                    state['step'] = 0\n",
            "                    # Exponential moving average of gradient values\n",
            "                    state['exp_avg'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    # Exponential moving average of squared gradient values\n",
            "                    state['exp_avg_var'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    \n",
            "                    # Last gradients\n",
            "                    state['last_grad'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    # Last weights\n",
            "                    state['last_weight'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                        if version_higher else torch.zeros_like(p.data)\n",
            "                    if amsgrad:\n",
            "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
            "                        state['max_exp_avg_var'] = torch.zeros_like(p.data,memory_format=torch.preserve_format) \\\n",
            "                            if version_higher else torch.zeros_like(p.data)\n",
            "\n",
            "                # get current state variable\n",
            "                exp_avg, last_grad, exp_avg_var, last_weight = state['exp_avg'], state['last_grad'], state['exp_avg_var'], state['last_weight']\n",
            "\n",
            "                state['step'] += 1\n",
            "\n",
            "                # Approximate Nesterov's Accelerated Gradient\n",
            "                exp_avg.mul_(beta1).add_(grad)\n",
            "                grad.add_(grad - last_grad, alpha=beta1)\n",
            "                last_grad = p.grad.data\n",
            "                exp_avg_var.mul_(beta2).addcmul_(grad, grad)\n",
            "\n",
            "                if amsgrad:\n",
            "                    max_exp_avg_var = state['max_exp_avg_var']\n",
            "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
            "                    torch.max(max_exp_avg_var, exp_avg_var, out=max_exp_avg_var)\n",
            "\n",
            "                    # Use the max. for normalizing running avg. of gradient\n",
            "                    denom = max_exp_avg_var.sqrt().add_(group['eps'])\n",
            "                else:\n",
            "                    denom = exp_avg_var.add_(group['eps'])\n",
            "\n",
            "                # perform weight decay, check if decoupled weight decay\n",
            "                if self.weight_decouple:\n",
            "                    if not self.fixed_decay:\n",
            "                        p.data.mul_(1.0 - group['lr'] * group['weight_decay'])\n",
            "                    else:\n",
            "                        p.data.mul_(1.0 - group['weight_decay'])\n",
            "                else:\n",
            "                    if group['weight_decay'] != 0:\n",
            "                        grad.add_(p.data, alpha=group['weight_decay'])\n",
            "\n",
            "                # Update\n",
            "                tmp = p.data\n",
            "                p.data.add_(p.data - last_weight, alpha=beta1)\n",
            "                p.data.addcdiv_(exp_avg, denom, value=-group['lr'])\n",
            "                last_weight = tmp\n",
            "\n",
            "        return loss\n",
            "model = models.resnet34()\n",
            "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
            "criterion = nn.CrossEntropyLoss()\n",
            "n_epochs = 50\n",
            "nadam_train_list = None\n",
            "nadam_test_list = None\n",
            "optimizer = NAdam(model.parameters())\n",
            "model.to(device)\n",
            "path = 'nadamRes.pt'\n",
            "nadam_train_list, nadam_test_list = trainImageModel(model, trainloader, testloader,\n",
            "                                                                    n_epochs, criterion, optimizer,\n",
            "                                                                    device, path)\n",
            "torch.save(nadam_train_list, 'nadamres_train.pt')\n",
            "torch.save(nadam_test_list, 'nadamres_test.pt')\n",
            "%history\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRAGqFcZVto6"
      },
      "source": [
        "version_higher = (torch.__version__ >= \"1.5.0\")\n",
        "\n",
        "class NAdam(Optimizer):\n",
        "    def __init__(self, params, lr=1e-03, betas=(0.9, 0.999), eps=1e-16,\n",
        "                 weight_decay=0, amsgrad=False, weight_decouple=True, fixed_decay=False):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        if not 0.0 <= weight_decay:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
        "            for param in params:\n",
        "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
        "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
        "\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay, amsgrad=amsgrad, buffer=[[None, None, None] for _ in range(10)])\n",
        "        super(NAdam, self).__init__(params, defaults)\n",
        "\n",
        "        self.weight_decouple = weight_decouple\n",
        "        self.fixed_decay = fixed_decay\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(NAdam, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('amsgrad', False)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError(\n",
        "                        'NAdam does not support sparse gradients, please consider SparseAdam instead')\n",
        "                amsgrad = group['amsgrad']\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data, memory_format=torch.preserve_format) \\\n",
        "                        if version_higher else torch.zeros_like(p.data)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_var'] = torch.zeros_like(p.data, memory_format=torch.preserve_format) \\\n",
        "                        if version_higher else torch.zeros_like(p.data)\n",
        "\n",
        "                    if amsgrad:\n",
        "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
        "                        state['max_exp_avg_var'] = torch.zeros_like(p.data, memory_format=torch.preserve_format) \\\n",
        "                            if version_higher else torch.zeros_like(p.data)\n",
        "\n",
        "                # get current state variable\n",
        "                exp_avg, exp_avg_var = state['exp_avg'], state['exp_avg_var']\n",
        "\n",
        "                state['step'] += 1\n",
        "                bias_correction1 = 1 - beta1 ** state['step']\n",
        "                bias_correction2 = 1 - beta2 ** state['step']\n",
        "\n",
        "                # Approximate Nesterov's Accelerated Gradient\n",
        "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "                grad_diff = grad - exp_avg\n",
        "                exp_avg_var.mul_(beta2).addcmul_(grad_diff, grad_diff, value=1 - beta2)\n",
        "                grad.mul_(1 - beta1).add_(exp_avg, alpha=beta1)\n",
        "\n",
        "                if amsgrad:\n",
        "                    max_exp_avg_var = state['max_exp_avg_var']\n",
        "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
        "                    torch.max(max_exp_avg_var, exp_avg_var, out=max_exp_avg_var)\n",
        "\n",
        "                    # Use the max. for normalizing running avg. of gradient\n",
        "                    denom = (max_exp_avg_var.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
        "\n",
        "                else:\n",
        "                    denom = (exp_avg_var.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
        "\n",
        "                # perform weight decay, check if decoupled weight decay\n",
        "                if self.weight_decouple:\n",
        "                    if not self.fixed_decay:\n",
        "                        p.data.mul_(1.0 - group['lr'] * group['weight_decay'])\n",
        "                    else:\n",
        "                        p.data.mul_(1.0 - group['weight_decay'])\n",
        "                else:\n",
        "                    if group['weight_decay'] != 0:\n",
        "                        grad.add_(p.data, alpha=group['weight_decay'])\n",
        "\n",
        "                # Update\n",
        "                step_size = group['lr'] / bias_correction1          \n",
        "                p.data.addcdiv_(grad, denom, value=-step_size)\n",
        "\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idIPQkk_Ic2y"
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYtO9TD5IsWG"
      },
      "source": [
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "train_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                      transforms.RandomResizedCrop(224), \n",
        "                                      transforms.RandomHorizontalFlip(), \n",
        "                                      transforms.ToTensor(),\n",
        "                                      normalize])\n",
        "test_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                     transforms.CenterCrop(224),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     normalize])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "38bd9a76f83f4d0b8d8a62d9cd526c93",
            "5ee54d04dee840d1b3addd8609d0083d",
            "229c18af8b104884b59dafb81a1c6db5",
            "85b7a96f345f4892b1f25f61c07a33dc",
            "26249bdb2af845e18b3d81d3f2c4eaa3",
            "c662d96a379c463ea32e81cd595d3aad",
            "16bcbc72b3234a829f5119c2acdfe958",
            "d98728c067ac4a80a6e2701f8c1b3e8c"
          ]
        },
        "id": "pYX_Z4qaIuyJ",
        "outputId": "e7f945f3-9bc6-4248-9356-9000c4e84e7d"
      },
      "source": [
        "batch_size = 100\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=train_transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=test_transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "38bd9a76f83f4d0b8d8a62d9cd526c93",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUI8U0FNIwn3"
      },
      "source": [
        "import time\n",
        "\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = s // 60\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD22rDMtIzQl"
      },
      "source": [
        "import torchvision.models as models"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqKEZsWEI1Ll"
      },
      "source": [
        "def evaluateImageModel(model, data, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for image, labels in data:\n",
        "            image, labels = image.to(device), labels.to(device)\n",
        "            output = model(image)\n",
        "            pred = torch.argmax(output, dim=1)\n",
        "            correct += (pred == labels).sum().item()\n",
        "            total += len(labels)\n",
        "            loss = criterion(output, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(data), 1 - correct / total\n",
        "\n",
        "def trainImageModel(model, train_data, val_data, n_epochs, criterion, optimizer, device, path):\n",
        "    model.train()\n",
        "    train_error_list = []\n",
        "    val_error_list = []\n",
        "    min_error = None\n",
        "    step = 0\n",
        "    print_every = len(train_data)\n",
        "    start = time.time()\n",
        "    for epoch in range(n_epochs):\n",
        "        running_loss = 0.0\n",
        "        running_correct = 0\n",
        "        running_total = 0\n",
        "        for image, labels in train_data:\n",
        "            optimizer.zero_grad()\n",
        "            step += 1\n",
        "            image, labels = image.to(device), labels.to(device)\n",
        "            output = model(image)\n",
        "            pred = torch.argmax(output, dim=1)\n",
        "            running_correct += (pred == labels).sum().item()\n",
        "            running_total += len(labels)\n",
        "            loss = criterion(output, labels)\n",
        "            running_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if step % print_every == 0:\n",
        "                val_loss, val_error = evaluateImageModel(model, val_data, criterion, device)\n",
        "                print(('%d/%d (%s) train loss: %.3f, train error: %.2f%%, val loss: %.3f, val error: %.2f%%') %\n",
        "                      (epoch + 1, n_epochs, timeSince(start), running_loss / len(train_data), \n",
        "                       100 *(1 - running_correct / running_total), val_loss, 100 * val_error))\n",
        "                train_error_list.append(1 - running_correct / running_total)\n",
        "                val_error_list.append(val_error)\n",
        "                if min_error is None or min_error > val_error:\n",
        "                    if min_error is None:\n",
        "                        print(('Validation error rate in first epoch: %.2f%%') % (100 * val_error))\n",
        "                    else:\n",
        "                        print(('Validation error rate is decreasing: %.2f%% --> %.2f%%') % \n",
        "                              (100 * min_error, 100 * val_error))\n",
        "                    min_error = val_error\n",
        "                    print('Saving model...')\n",
        "                    torch.save(model, path)\n",
        "                \n",
        "                model.train()\n",
        "                running_loss = 0.0\n",
        "                running_correct = 0\n",
        "                running_total = 0\n",
        "    \n",
        "    return train_error_list, val_error_list"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlcvumX_YjH0",
        "outputId": "30edb5d2-d276-41bc-a817-00a852649f6e"
      },
      "source": [
        "model = models.resnet34()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "n_epochs = 50\n",
        "nadam_train_list = None\n",
        "nadam_test_list = None\n",
        "optimizer = NAdam(model.parameters())\n",
        "model.to(device)\n",
        "path = 'nadamRes.pt'\n",
        "nadam_train_list, nadam_test_list = trainImageModel(model, trainloader, testloader,\n",
        "                                                    n_epochs, criterion, optimizer,\n",
        "                                                    device, path)\n",
        "torch.save(nadam_train_list, 'nadamres_train.pt')\n",
        "torch.save(nadam_test_list, 'nadamres_test.pt')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/50 (1m 51s) train loss: 1.811, train error: 65.89%, val loss: 1.389, val error: 50.26%\n",
            "Validation error rate in first epoch: 50.26%\n",
            "Saving model...\n",
            "2/50 (3m 43s) train loss: 1.417, train error: 51.06%, val loss: 1.057, val error: 37.07%\n",
            "Validation error rate is decreasing: 50.26% --> 37.07%\n",
            "Saving model...\n",
            "3/50 (5m 35s) train loss: 1.205, train error: 43.01%, val loss: 0.983, val error: 34.37%\n",
            "Validation error rate is decreasing: 37.07% --> 34.37%\n",
            "Saving model...\n",
            "4/50 (7m 28s) train loss: 1.068, train error: 37.89%, val loss: 0.837, val error: 28.64%\n",
            "Validation error rate is decreasing: 34.37% --> 28.64%\n",
            "Saving model...\n",
            "5/50 (9m 21s) train loss: 0.971, train error: 34.19%, val loss: 0.661, val error: 23.64%\n",
            "Validation error rate is decreasing: 28.64% --> 23.64%\n",
            "Saving model...\n",
            "6/50 (11m 14s) train loss: 0.907, train error: 31.90%, val loss: 0.694, val error: 23.95%\n",
            "7/50 (13m 7s) train loss: 0.859, train error: 30.23%, val loss: 0.566, val error: 19.69%\n",
            "Validation error rate is decreasing: 23.64% --> 19.69%\n",
            "Saving model...\n",
            "8/50 (14m 59s) train loss: 0.818, train error: 28.72%, val loss: 0.536, val error: 18.32%\n",
            "Validation error rate is decreasing: 19.69% --> 18.32%\n",
            "Saving model...\n",
            "9/50 (16m 51s) train loss: 0.777, train error: 27.35%, val loss: 0.488, val error: 16.79%\n",
            "Validation error rate is decreasing: 18.32% --> 16.79%\n",
            "Saving model...\n",
            "10/50 (18m 44s) train loss: 0.737, train error: 25.71%, val loss: 0.460, val error: 15.25%\n",
            "Validation error rate is decreasing: 16.79% --> 15.25%\n",
            "Saving model...\n",
            "11/50 (20m 36s) train loss: 0.709, train error: 24.67%, val loss: 0.442, val error: 15.15%\n",
            "Validation error rate is decreasing: 15.25% --> 15.15%\n",
            "Saving model...\n",
            "12/50 (22m 29s) train loss: 0.683, train error: 23.81%, val loss: 0.464, val error: 16.21%\n",
            "13/50 (24m 21s) train loss: 0.657, train error: 23.10%, val loss: 0.437, val error: 14.64%\n",
            "Validation error rate is decreasing: 15.15% --> 14.64%\n",
            "Saving model...\n",
            "14/50 (26m 14s) train loss: 0.629, train error: 22.11%, val loss: 0.430, val error: 14.33%\n",
            "Validation error rate is decreasing: 14.64% --> 14.33%\n",
            "Saving model...\n",
            "15/50 (28m 6s) train loss: 0.616, train error: 21.43%, val loss: 0.429, val error: 14.28%\n",
            "Validation error rate is decreasing: 14.33% --> 14.28%\n",
            "Saving model...\n",
            "16/50 (29m 58s) train loss: 0.589, train error: 20.43%, val loss: 0.400, val error: 13.94%\n",
            "Validation error rate is decreasing: 14.28% --> 13.94%\n",
            "Saving model...\n",
            "17/50 (31m 51s) train loss: 0.577, train error: 20.24%, val loss: 0.360, val error: 12.05%\n",
            "Validation error rate is decreasing: 13.94% --> 12.05%\n",
            "Saving model...\n",
            "18/50 (33m 44s) train loss: 0.552, train error: 19.18%, val loss: 0.377, val error: 12.78%\n",
            "19/50 (35m 36s) train loss: 0.540, train error: 18.84%, val loss: 0.356, val error: 11.85%\n",
            "Validation error rate is decreasing: 12.05% --> 11.85%\n",
            "Saving model...\n",
            "20/50 (37m 28s) train loss: 0.528, train error: 18.23%, val loss: 0.322, val error: 11.07%\n",
            "Validation error rate is decreasing: 11.85% --> 11.07%\n",
            "Saving model...\n",
            "21/50 (39m 19s) train loss: 0.514, train error: 17.89%, val loss: 0.361, val error: 12.00%\n",
            "22/50 (41m 11s) train loss: 0.499, train error: 17.30%, val loss: 0.318, val error: 10.82%\n",
            "Validation error rate is decreasing: 11.07% --> 10.82%\n",
            "Saving model...\n",
            "23/50 (43m 3s) train loss: 0.493, train error: 17.12%, val loss: 0.312, val error: 10.34%\n",
            "Validation error rate is decreasing: 10.82% --> 10.34%\n",
            "Saving model...\n",
            "24/50 (44m 54s) train loss: 0.471, train error: 16.27%, val loss: 0.312, val error: 10.32%\n",
            "Validation error rate is decreasing: 10.34% --> 10.32%\n",
            "Saving model...\n",
            "25/50 (46m 46s) train loss: 0.466, train error: 16.27%, val loss: 0.336, val error: 10.78%\n",
            "26/50 (48m 38s) train loss: 0.453, train error: 15.84%, val loss: 0.329, val error: 11.16%\n",
            "27/50 (50m 29s) train loss: 0.447, train error: 15.66%, val loss: 0.309, val error: 9.80%\n",
            "Validation error rate is decreasing: 10.32% --> 9.80%\n",
            "Saving model...\n",
            "28/50 (52m 20s) train loss: 0.435, train error: 15.22%, val loss: 0.296, val error: 9.56%\n",
            "Validation error rate is decreasing: 9.80% --> 9.56%\n",
            "Saving model...\n",
            "29/50 (54m 13s) train loss: 0.426, train error: 14.85%, val loss: 0.314, val error: 10.45%\n",
            "30/50 (56m 4s) train loss: 0.421, train error: 14.66%, val loss: 0.294, val error: 9.46%\n",
            "Validation error rate is decreasing: 9.56% --> 9.46%\n",
            "Saving model...\n",
            "31/50 (57m 57s) train loss: 0.402, train error: 14.07%, val loss: 0.270, val error: 8.71%\n",
            "Validation error rate is decreasing: 9.46% --> 8.71%\n",
            "Saving model...\n",
            "32/50 (59m 48s) train loss: 0.398, train error: 13.88%, val loss: 0.280, val error: 9.02%\n",
            "33/50 (61m 39s) train loss: 0.391, train error: 13.73%, val loss: 0.279, val error: 8.71%\n",
            "34/50 (63m 30s) train loss: 0.384, train error: 13.56%, val loss: 0.277, val error: 8.35%\n",
            "Validation error rate is decreasing: 8.71% --> 8.35%\n",
            "Saving model...\n",
            "35/50 (65m 22s) train loss: 0.379, train error: 13.16%, val loss: 0.259, val error: 8.37%\n",
            "36/50 (67m 14s) train loss: 0.369, train error: 12.83%, val loss: 0.288, val error: 8.74%\n",
            "37/50 (69m 5s) train loss: 0.366, train error: 12.71%, val loss: 0.267, val error: 8.07%\n",
            "Validation error rate is decreasing: 8.35% --> 8.07%\n",
            "Saving model...\n",
            "38/50 (70m 57s) train loss: 0.359, train error: 12.49%, val loss: 0.286, val error: 9.04%\n",
            "39/50 (72m 48s) train loss: 0.351, train error: 12.18%, val loss: 0.280, val error: 8.29%\n",
            "40/50 (74m 40s) train loss: 0.346, train error: 12.06%, val loss: 0.258, val error: 7.78%\n",
            "Validation error rate is decreasing: 8.07% --> 7.78%\n",
            "Saving model...\n",
            "41/50 (76m 32s) train loss: 0.345, train error: 12.16%, val loss: 0.254, val error: 8.00%\n",
            "42/50 (78m 24s) train loss: 0.339, train error: 11.84%, val loss: 0.244, val error: 7.68%\n",
            "Validation error rate is decreasing: 7.78% --> 7.68%\n",
            "Saving model...\n",
            "43/50 (80m 14s) train loss: 0.334, train error: 11.59%, val loss: 0.263, val error: 7.78%\n",
            "44/50 (82m 4s) train loss: 0.327, train error: 11.34%, val loss: 0.266, val error: 7.44%\n",
            "Validation error rate is decreasing: 7.68% --> 7.44%\n",
            "Saving model...\n",
            "45/50 (83m 56s) train loss: 0.322, train error: 11.22%, val loss: 0.291, val error: 8.52%\n",
            "46/50 (85m 48s) train loss: 0.325, train error: 11.27%, val loss: 0.261, val error: 7.55%\n",
            "47/50 (87m 41s) train loss: 0.316, train error: 11.02%, val loss: 0.262, val error: 7.58%\n",
            "48/50 (89m 33s) train loss: 0.308, train error: 10.69%, val loss: 0.251, val error: 7.37%\n",
            "Validation error rate is decreasing: 7.44% --> 7.37%\n",
            "Saving model...\n",
            "49/50 (91m 25s) train loss: 0.310, train error: 10.90%, val loss: 0.259, val error: 7.40%\n",
            "50/50 (93m 17s) train loss: 0.306, train error: 10.58%, val loss: 0.238, val error: 6.86%\n",
            "Validation error rate is decreasing: 7.37% --> 6.86%\n",
            "Saving model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpspM--tpQj_",
        "outputId": "a460c200-fed0-4b50-859e-815fc2d6bc28"
      },
      "source": [
        "model = models.shufflenet_v2_x1_0()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "n_epochs = 50\n",
        "nadam_train_list = None\n",
        "nadam_test_list = None\n",
        "optimizer = NestAdam(model.parameters())\n",
        "model.to(device)\n",
        "path = 'nadamshffule.pt'\n",
        "nadam_train_list, nadam_test_list = trainImageModel(model, trainloader, testloader,\n",
        "                                                                    n_epochs, criterion, optimizer,\n",
        "                                                                    device, path)\n",
        "torch.save(nadam_train_list, 'nadamres_train.pt')\n",
        "torch.save(nadam_test_list, 'nadamres_test.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/50 (1m 38s) train loss: 1.724, train error: 62.83%, val loss: 1.336, val error: 48.49%\n",
            "Validation error rate in first epoch: 48.49%\n",
            "Saving model...\n",
            "2/50 (3m 15s) train loss: 1.385, train error: 49.66%, val loss: 1.076, val error: 38.44%\n",
            "Validation error rate is decreasing: 48.49% --> 38.44%\n",
            "Saving model...\n",
            "3/50 (4m 53s) train loss: 1.206, train error: 42.85%, val loss: 0.936, val error: 32.11%\n",
            "Validation error rate is decreasing: 38.44% --> 32.11%\n",
            "Saving model...\n",
            "4/50 (6m 30s) train loss: 1.090, train error: 38.65%, val loss: 0.754, val error: 26.63%\n",
            "Validation error rate is decreasing: 32.11% --> 26.63%\n",
            "Saving model...\n",
            "5/50 (8m 9s) train loss: 0.992, train error: 35.12%, val loss: 0.708, val error: 24.24%\n",
            "Validation error rate is decreasing: 26.63% --> 24.24%\n",
            "Saving model...\n",
            "6/50 (9m 48s) train loss: 0.931, train error: 32.55%, val loss: 0.705, val error: 24.38%\n",
            "7/50 (11m 29s) train loss: 0.876, train error: 30.60%, val loss: 0.585, val error: 19.93%\n",
            "Validation error rate is decreasing: 24.24% --> 19.93%\n",
            "Saving model...\n",
            "8/50 (13m 8s) train loss: 0.832, train error: 28.96%, val loss: 0.594, val error: 20.77%\n",
            "9/50 (14m 49s) train loss: 0.798, train error: 27.82%, val loss: 0.500, val error: 17.00%\n",
            "Validation error rate is decreasing: 19.93% --> 17.00%\n",
            "Saving model...\n",
            "10/50 (16m 29s) train loss: 0.763, train error: 26.64%, val loss: 0.501, val error: 17.26%\n",
            "11/50 (18m 8s) train loss: 0.744, train error: 25.95%, val loss: 0.500, val error: 17.23%\n",
            "12/50 (19m 49s) train loss: 0.723, train error: 25.51%, val loss: 0.491, val error: 16.53%\n",
            "Validation error rate is decreasing: 17.00% --> 16.53%\n",
            "Saving model...\n",
            "13/50 (21m 32s) train loss: 0.696, train error: 24.14%, val loss: 0.468, val error: 15.77%\n",
            "Validation error rate is decreasing: 16.53% --> 15.77%\n",
            "Saving model...\n",
            "14/50 (23m 14s) train loss: 0.675, train error: 23.24%, val loss: 0.436, val error: 14.85%\n",
            "Validation error rate is decreasing: 15.77% --> 14.85%\n",
            "Saving model...\n",
            "15/50 (24m 57s) train loss: 0.664, train error: 23.13%, val loss: 0.402, val error: 13.49%\n",
            "Validation error rate is decreasing: 14.85% --> 13.49%\n",
            "Saving model...\n",
            "16/50 (26m 38s) train loss: 0.645, train error: 22.43%, val loss: 0.408, val error: 14.27%\n",
            "17/50 (28m 20s) train loss: 0.626, train error: 21.69%, val loss: 0.427, val error: 14.37%\n",
            "18/50 (30m 1s) train loss: 0.621, train error: 21.63%, val loss: 0.375, val error: 12.58%\n",
            "Validation error rate is decreasing: 13.49% --> 12.58%\n",
            "Saving model...\n",
            "19/50 (31m 44s) train loss: 0.605, train error: 20.95%, val loss: 0.387, val error: 13.40%\n",
            "20/50 (33m 26s) train loss: 0.592, train error: 20.70%, val loss: 0.357, val error: 11.94%\n",
            "Validation error rate is decreasing: 12.58% --> 11.94%\n",
            "Saving model...\n",
            "21/50 (35m 8s) train loss: 0.578, train error: 20.10%, val loss: 0.353, val error: 11.79%\n",
            "Validation error rate is decreasing: 11.94% --> 11.79%\n",
            "Saving model...\n",
            "22/50 (36m 50s) train loss: 0.564, train error: 19.72%, val loss: 0.390, val error: 12.89%\n",
            "23/50 (38m 30s) train loss: 0.562, train error: 19.63%, val loss: 0.368, val error: 12.53%\n",
            "24/50 (40m 11s) train loss: 0.553, train error: 19.05%, val loss: 0.344, val error: 11.71%\n",
            "Validation error rate is decreasing: 11.79% --> 11.71%\n",
            "Saving model...\n",
            "25/50 (41m 52s) train loss: 0.542, train error: 18.90%, val loss: 0.327, val error: 11.05%\n",
            "Validation error rate is decreasing: 11.71% --> 11.05%\n",
            "Saving model...\n",
            "26/50 (43m 33s) train loss: 0.537, train error: 18.70%, val loss: 0.333, val error: 11.14%\n",
            "27/50 (45m 16s) train loss: 0.531, train error: 18.70%, val loss: 0.352, val error: 11.71%\n",
            "28/50 (46m 59s) train loss: 0.526, train error: 18.55%, val loss: 0.322, val error: 10.88%\n",
            "Validation error rate is decreasing: 11.05% --> 10.88%\n",
            "Saving model...\n",
            "29/50 (48m 40s) train loss: 0.513, train error: 17.85%, val loss: 0.300, val error: 10.01%\n",
            "Validation error rate is decreasing: 10.88% --> 10.01%\n",
            "Saving model...\n",
            "30/50 (50m 23s) train loss: 0.508, train error: 17.74%, val loss: 0.323, val error: 10.73%\n",
            "31/50 (52m 4s) train loss: 0.501, train error: 17.59%, val loss: 0.302, val error: 10.21%\n",
            "32/50 (53m 46s) train loss: 0.495, train error: 17.39%, val loss: 0.302, val error: 10.02%\n",
            "33/50 (55m 27s) train loss: 0.482, train error: 16.66%, val loss: 0.304, val error: 10.26%\n",
            "34/50 (57m 12s) train loss: 0.480, train error: 16.59%, val loss: 0.286, val error: 9.77%\n",
            "Validation error rate is decreasing: 10.01% --> 9.77%\n",
            "Saving model...\n",
            "35/50 (58m 54s) train loss: 0.477, train error: 16.70%, val loss: 0.290, val error: 9.82%\n",
            "36/50 (60m 35s) train loss: 0.467, train error: 16.42%, val loss: 0.289, val error: 9.86%\n",
            "37/50 (62m 16s) train loss: 0.464, train error: 16.12%, val loss: 0.275, val error: 9.08%\n",
            "Validation error rate is decreasing: 9.77% --> 9.08%\n",
            "Saving model...\n",
            "38/50 (63m 57s) train loss: 0.463, train error: 16.09%, val loss: 0.282, val error: 9.74%\n",
            "39/50 (65m 37s) train loss: 0.458, train error: 16.02%, val loss: 0.285, val error: 9.37%\n",
            "40/50 (67m 17s) train loss: 0.444, train error: 15.41%, val loss: 0.299, val error: 9.54%\n",
            "41/50 (68m 55s) train loss: 0.446, train error: 15.43%, val loss: 0.298, val error: 9.83%\n",
            "42/50 (70m 35s) train loss: 0.433, train error: 15.06%, val loss: 0.280, val error: 9.23%\n",
            "43/50 (72m 16s) train loss: 0.433, train error: 15.14%, val loss: 0.307, val error: 10.34%\n",
            "44/50 (73m 54s) train loss: 0.433, train error: 15.21%, val loss: 0.285, val error: 9.32%\n",
            "45/50 (75m 32s) train loss: 0.428, train error: 15.05%, val loss: 0.282, val error: 9.27%\n",
            "46/50 (77m 10s) train loss: 0.427, train error: 14.94%, val loss: 0.273, val error: 9.19%\n",
            "47/50 (78m 50s) train loss: 0.424, train error: 14.83%, val loss: 0.282, val error: 9.27%\n",
            "48/50 (80m 31s) train loss: 0.409, train error: 14.17%, val loss: 0.270, val error: 9.18%\n",
            "49/50 (82m 11s) train loss: 0.412, train error: 14.50%, val loss: 0.289, val error: 9.38%\n",
            "50/50 (83m 53s) train loss: 0.406, train error: 14.18%, val loss: 0.287, val error: 9.32%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VwTFtgdVspg3",
        "outputId": "7684ad50-620c-4445-a1f0-68920cbfc724"
      },
      "source": [
        "model = models.mobilenet_v2()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "n_epochs = 50\n",
        "nadam_train_list = None\n",
        "nadam_test_list = None\n",
        "optimizer = NestAdam(model.parameters())\n",
        "model.to(device)\n",
        "name = 'nadammobile'\n",
        "path = name + '.pt'\n",
        "nadam_train_list, nadam_test_list = trainImageModel(model, trainloader, testloader,\n",
        "                                                                    n_epochs, criterion, optimizer,\n",
        "                                                                    device, path)\n",
        "torch.save(nadam_train_list, name + '_train.pt')\n",
        "torch.save(nadam_test_list, name + '_test.pt')\n",
        "files.download(name + '_train.pt')\n",
        "files.download(name + '_test.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/50 (1m 46s) train loss: 1.680, train error: 61.48%, val loss: 1.416, val error: 49.14%\n",
            "Validation error rate in first epoch: 49.14%\n",
            "Saving model...\n",
            "2/50 (3m 31s) train loss: 1.363, train error: 48.80%, val loss: 1.098, val error: 39.37%\n",
            "Validation error rate is decreasing: 49.14% --> 39.37%\n",
            "Saving model...\n",
            "3/50 (5m 17s) train loss: 1.197, train error: 42.66%, val loss: 0.857, val error: 30.29%\n",
            "Validation error rate is decreasing: 39.37% --> 30.29%\n",
            "Saving model...\n",
            "4/50 (7m 3s) train loss: 1.080, train error: 38.18%, val loss: 0.764, val error: 26.89%\n",
            "Validation error rate is decreasing: 30.29% --> 26.89%\n",
            "Saving model...\n",
            "5/50 (8m 48s) train loss: 1.000, train error: 35.42%, val loss: 0.713, val error: 24.21%\n",
            "Validation error rate is decreasing: 26.89% --> 24.21%\n",
            "Saving model...\n",
            "6/50 (10m 32s) train loss: 0.939, train error: 33.15%, val loss: 0.609, val error: 21.11%\n",
            "Validation error rate is decreasing: 24.21% --> 21.11%\n",
            "Saving model...\n",
            "7/50 (12m 17s) train loss: 0.889, train error: 31.13%, val loss: 0.608, val error: 20.88%\n",
            "Validation error rate is decreasing: 21.11% --> 20.88%\n",
            "Saving model...\n",
            "8/50 (14m 3s) train loss: 0.848, train error: 29.79%, val loss: 0.537, val error: 18.85%\n",
            "Validation error rate is decreasing: 20.88% --> 18.85%\n",
            "Saving model...\n",
            "9/50 (15m 50s) train loss: 0.824, train error: 28.79%, val loss: 0.496, val error: 17.18%\n",
            "Validation error rate is decreasing: 18.85% --> 17.18%\n",
            "Saving model...\n",
            "10/50 (17m 38s) train loss: 0.787, train error: 27.62%, val loss: 0.579, val error: 18.74%\n",
            "11/50 (19m 23s) train loss: 0.758, train error: 26.49%, val loss: 0.504, val error: 17.42%\n",
            "12/50 (21m 8s) train loss: 0.732, train error: 25.33%, val loss: 0.546, val error: 18.55%\n",
            "13/50 (22m 54s) train loss: 0.713, train error: 25.03%, val loss: 0.494, val error: 16.56%\n",
            "Validation error rate is decreasing: 17.18% --> 16.56%\n",
            "Saving model...\n",
            "14/50 (24m 39s) train loss: 0.690, train error: 23.90%, val loss: 0.426, val error: 14.79%\n",
            "Validation error rate is decreasing: 16.56% --> 14.79%\n",
            "Saving model...\n",
            "15/50 (26m 24s) train loss: 0.677, train error: 23.56%, val loss: 0.427, val error: 14.60%\n",
            "Validation error rate is decreasing: 14.79% --> 14.60%\n",
            "Saving model...\n",
            "16/50 (28m 11s) train loss: 0.655, train error: 22.89%, val loss: 0.410, val error: 14.00%\n",
            "Validation error rate is decreasing: 14.60% --> 14.00%\n",
            "Saving model...\n",
            "17/50 (29m 58s) train loss: 0.643, train error: 22.41%, val loss: 0.395, val error: 13.69%\n",
            "Validation error rate is decreasing: 14.00% --> 13.69%\n",
            "Saving model...\n",
            "18/50 (31m 48s) train loss: 0.635, train error: 22.27%, val loss: 0.387, val error: 13.33%\n",
            "Validation error rate is decreasing: 13.69% --> 13.33%\n",
            "Saving model...\n",
            "19/50 (33m 39s) train loss: 0.617, train error: 21.56%, val loss: 0.390, val error: 12.86%\n",
            "Validation error rate is decreasing: 13.33% --> 12.86%\n",
            "Saving model...\n",
            "20/50 (35m 26s) train loss: 0.601, train error: 20.87%, val loss: 0.414, val error: 13.75%\n",
            "21/50 (37m 15s) train loss: 0.588, train error: 20.57%, val loss: 0.349, val error: 11.85%\n",
            "Validation error rate is decreasing: 12.86% --> 11.85%\n",
            "Saving model...\n",
            "22/50 (39m 4s) train loss: 0.578, train error: 20.20%, val loss: 0.360, val error: 12.25%\n",
            "23/50 (40m 51s) train loss: 0.568, train error: 19.90%, val loss: 0.330, val error: 11.49%\n",
            "Validation error rate is decreasing: 11.85% --> 11.49%\n",
            "Saving model...\n",
            "24/50 (42m 39s) train loss: 0.561, train error: 19.59%, val loss: 0.366, val error: 12.44%\n",
            "25/50 (44m 27s) train loss: 0.548, train error: 19.00%, val loss: 0.322, val error: 10.85%\n",
            "Validation error rate is decreasing: 11.49% --> 10.85%\n",
            "Saving model...\n",
            "26/50 (46m 14s) train loss: 0.543, train error: 18.82%, val loss: 0.311, val error: 10.44%\n",
            "Validation error rate is decreasing: 10.85% --> 10.44%\n",
            "Saving model...\n",
            "27/50 (48m 0s) train loss: 0.527, train error: 18.41%, val loss: 0.335, val error: 11.19%\n",
            "28/50 (49m 47s) train loss: 0.531, train error: 18.42%, val loss: 0.329, val error: 10.95%\n",
            "29/50 (51m 34s) train loss: 0.518, train error: 18.11%, val loss: 0.314, val error: 10.66%\n",
            "30/50 (53m 20s) train loss: 0.514, train error: 17.92%, val loss: 0.309, val error: 10.19%\n",
            "Validation error rate is decreasing: 10.44% --> 10.19%\n",
            "Saving model...\n",
            "31/50 (55m 9s) train loss: 0.505, train error: 17.42%, val loss: 0.316, val error: 10.76%\n",
            "32/50 (56m 53s) train loss: 0.498, train error: 17.28%, val loss: 0.304, val error: 10.03%\n",
            "Validation error rate is decreasing: 10.19% --> 10.03%\n",
            "Saving model...\n",
            "33/50 (58m 38s) train loss: 0.493, train error: 17.03%, val loss: 0.307, val error: 10.40%\n",
            "34/50 (60m 24s) train loss: 0.493, train error: 17.18%, val loss: 0.358, val error: 11.71%\n",
            "35/50 (62m 11s) train loss: 0.484, train error: 17.03%, val loss: 0.318, val error: 10.71%\n",
            "36/50 (63m 58s) train loss: 0.471, train error: 16.44%, val loss: 0.277, val error: 9.13%\n",
            "Validation error rate is decreasing: 10.03% --> 9.13%\n",
            "Saving model...\n",
            "37/50 (65m 44s) train loss: 0.462, train error: 16.12%, val loss: 0.303, val error: 9.99%\n",
            "38/50 (67m 30s) train loss: 0.463, train error: 16.04%, val loss: 0.294, val error: 9.52%\n",
            "39/50 (69m 16s) train loss: 0.457, train error: 15.89%, val loss: 0.285, val error: 9.55%\n",
            "40/50 (71m 1s) train loss: 0.455, train error: 15.87%, val loss: 0.278, val error: 9.11%\n",
            "Validation error rate is decreasing: 9.13% --> 9.11%\n",
            "Saving model...\n",
            "41/50 (72m 49s) train loss: 0.449, train error: 15.63%, val loss: 0.273, val error: 8.88%\n",
            "Validation error rate is decreasing: 9.11% --> 8.88%\n",
            "Saving model...\n",
            "42/50 (74m 35s) train loss: 0.445, train error: 15.54%, val loss: 0.276, val error: 9.35%\n",
            "43/50 (76m 19s) train loss: 0.439, train error: 15.35%, val loss: 0.276, val error: 8.99%\n",
            "44/50 (78m 3s) train loss: 0.434, train error: 15.22%, val loss: 0.287, val error: 9.53%\n",
            "45/50 (79m 51s) train loss: 0.433, train error: 14.90%, val loss: 0.270, val error: 9.04%\n",
            "46/50 (81m 37s) train loss: 0.425, train error: 14.89%, val loss: 0.280, val error: 9.18%\n",
            "47/50 (83m 26s) train loss: 0.416, train error: 14.61%, val loss: 0.278, val error: 8.84%\n",
            "Validation error rate is decreasing: 8.88% --> 8.84%\n",
            "Saving model...\n",
            "48/50 (85m 12s) train loss: 0.423, train error: 14.70%, val loss: 0.272, val error: 8.66%\n",
            "Validation error rate is decreasing: 8.84% --> 8.66%\n",
            "Saving model...\n",
            "49/50 (86m 58s) train loss: 0.415, train error: 14.40%, val loss: 0.280, val error: 9.33%\n",
            "50/50 (88m 46s) train loss: 0.410, train error: 14.33%, val loss: 0.254, val error: 8.76%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_3447568e-d0a1-4497-a035-48a7f545a04a\", \"nadammobile_train.pt\", 879)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_4201181e-c688-4681-b12c-3724100c36f6\", \"nadammobile_test.pt\", 879)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vVkIl3KrJjSj",
        "outputId": "0e5e7f59-16c8-473e-d224-8d3f44029214"
      },
      "source": [
        "model = models.densenet121()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "n_epochs = 50\n",
        "nadam_train_list = None\n",
        "nadam_test_list = None\n",
        "optimizer = NestAdam(model.parameters())\n",
        "model.to(device)\n",
        "name = 'nadamdense'\n",
        "path = name + '.pt'\n",
        "nadam_train_list, nadam_test_list = trainImageModel(model, trainloader, testloader,\n",
        "                                                                    n_epochs, criterion, optimizer,\n",
        "                                                                    device, path)\n",
        "torch.save(nadam_train_list, name + '_train.pt')\n",
        "torch.save(nadam_test_list, name + '_test.pt')\n",
        "files.download(name + '_train.pt')\n",
        "files.download(name + '_test.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/50 (3m 33s) train loss: 1.834, train error: 64.05%, val loss: 1.518, val error: 51.73%\n",
            "Validation error rate in first epoch: 51.73%\n",
            "Saving model...\n",
            "2/50 (7m 5s) train loss: 1.348, train error: 48.44%, val loss: 1.137, val error: 40.16%\n",
            "Validation error rate is decreasing: 51.73% --> 40.16%\n",
            "Saving model...\n",
            "3/50 (10m 38s) train loss: 1.148, train error: 40.95%, val loss: 0.827, val error: 28.91%\n",
            "Validation error rate is decreasing: 40.16% --> 28.91%\n",
            "Saving model...\n",
            "4/50 (14m 10s) train loss: 1.015, train error: 35.71%, val loss: 0.720, val error: 24.51%\n",
            "Validation error rate is decreasing: 28.91% --> 24.51%\n",
            "Saving model...\n",
            "5/50 (17m 42s) train loss: 0.924, train error: 32.40%, val loss: 0.605, val error: 21.36%\n",
            "Validation error rate is decreasing: 24.51% --> 21.36%\n",
            "Saving model...\n",
            "6/50 (21m 14s) train loss: 0.855, train error: 30.05%, val loss: 0.567, val error: 19.41%\n",
            "Validation error rate is decreasing: 21.36% --> 19.41%\n",
            "Saving model...\n",
            "7/50 (24m 46s) train loss: 0.803, train error: 28.03%, val loss: 0.601, val error: 20.64%\n",
            "8/50 (28m 18s) train loss: 0.756, train error: 26.48%, val loss: 0.591, val error: 19.97%\n",
            "9/50 (31m 50s) train loss: 0.721, train error: 25.22%, val loss: 0.451, val error: 15.55%\n",
            "Validation error rate is decreasing: 19.41% --> 15.55%\n",
            "Saving model...\n",
            "10/50 (35m 23s) train loss: 0.687, train error: 24.08%, val loss: 0.405, val error: 13.47%\n",
            "Validation error rate is decreasing: 15.55% --> 13.47%\n",
            "Saving model...\n",
            "11/50 (38m 57s) train loss: 0.660, train error: 22.96%, val loss: 0.426, val error: 14.34%\n",
            "12/50 (42m 31s) train loss: 0.632, train error: 21.96%, val loss: 0.431, val error: 14.10%\n",
            "13/50 (46m 5s) train loss: 0.611, train error: 21.26%, val loss: 0.369, val error: 12.29%\n",
            "Validation error rate is decreasing: 13.47% --> 12.29%\n",
            "Saving model...\n",
            "14/50 (49m 39s) train loss: 0.594, train error: 20.65%, val loss: 0.362, val error: 11.91%\n",
            "Validation error rate is decreasing: 12.29% --> 11.91%\n",
            "Saving model...\n",
            "15/50 (53m 13s) train loss: 0.570, train error: 19.93%, val loss: 0.375, val error: 12.52%\n",
            "16/50 (56m 47s) train loss: 0.554, train error: 19.37%, val loss: 0.335, val error: 11.11%\n",
            "Validation error rate is decreasing: 11.91% --> 11.11%\n",
            "Saving model...\n",
            "17/50 (60m 20s) train loss: 0.541, train error: 19.14%, val loss: 0.332, val error: 11.47%\n",
            "18/50 (63m 55s) train loss: 0.527, train error: 18.36%, val loss: 0.338, val error: 11.27%\n",
            "19/50 (67m 29s) train loss: 0.509, train error: 17.91%, val loss: 0.309, val error: 10.35%\n",
            "Validation error rate is decreasing: 11.11% --> 10.35%\n",
            "Saving model...\n",
            "20/50 (71m 4s) train loss: 0.497, train error: 17.42%, val loss: 0.309, val error: 10.57%\n",
            "21/50 (74m 38s) train loss: 0.483, train error: 16.92%, val loss: 0.322, val error: 10.50%\n",
            "22/50 (78m 12s) train loss: 0.466, train error: 16.23%, val loss: 0.311, val error: 10.20%\n",
            "Validation error rate is decreasing: 10.35% --> 10.20%\n",
            "Saving model...\n",
            "23/50 (81m 46s) train loss: 0.457, train error: 15.96%, val loss: 0.303, val error: 10.34%\n",
            "24/50 (85m 20s) train loss: 0.454, train error: 15.96%, val loss: 0.276, val error: 9.44%\n",
            "Validation error rate is decreasing: 10.20% --> 9.44%\n",
            "Saving model...\n",
            "25/50 (88m 55s) train loss: 0.446, train error: 15.56%, val loss: 0.297, val error: 9.45%\n",
            "26/50 (92m 29s) train loss: 0.435, train error: 15.15%, val loss: 0.270, val error: 9.08%\n",
            "Validation error rate is decreasing: 9.44% --> 9.08%\n",
            "Saving model...\n",
            "27/50 (96m 3s) train loss: 0.423, train error: 14.75%, val loss: 0.273, val error: 8.93%\n",
            "Validation error rate is decreasing: 9.08% --> 8.93%\n",
            "Saving model...\n",
            "28/50 (99m 37s) train loss: 0.421, train error: 14.62%, val loss: 0.265, val error: 8.50%\n",
            "Validation error rate is decreasing: 8.93% --> 8.50%\n",
            "Saving model...\n",
            "29/50 (103m 11s) train loss: 0.411, train error: 14.35%, val loss: 0.262, val error: 8.91%\n",
            "30/50 (106m 45s) train loss: 0.404, train error: 14.28%, val loss: 0.292, val error: 9.15%\n",
            "31/50 (110m 19s) train loss: 0.390, train error: 13.69%, val loss: 0.266, val error: 8.88%\n",
            "32/50 (113m 54s) train loss: 0.391, train error: 13.74%, val loss: 0.254, val error: 8.45%\n",
            "Validation error rate is decreasing: 8.50% --> 8.45%\n",
            "Saving model...\n",
            "33/50 (117m 28s) train loss: 0.383, train error: 13.26%, val loss: 0.305, val error: 9.75%\n",
            "34/50 (121m 2s) train loss: 0.383, train error: 13.42%, val loss: 0.262, val error: 8.44%\n",
            "Validation error rate is decreasing: 8.45% --> 8.44%\n",
            "Saving model...\n",
            "35/50 (124m 36s) train loss: 0.370, train error: 12.93%, val loss: 0.255, val error: 8.12%\n",
            "Validation error rate is decreasing: 8.44% --> 8.12%\n",
            "Saving model...\n",
            "36/50 (128m 11s) train loss: 0.367, train error: 12.74%, val loss: 0.251, val error: 7.77%\n",
            "Validation error rate is decreasing: 8.12% --> 7.77%\n",
            "Saving model...\n",
            "37/50 (131m 45s) train loss: 0.359, train error: 12.56%, val loss: 0.256, val error: 7.71%\n",
            "Validation error rate is decreasing: 7.77% --> 7.71%\n",
            "Saving model...\n",
            "38/50 (135m 19s) train loss: 0.358, train error: 12.43%, val loss: 0.239, val error: 7.42%\n",
            "Validation error rate is decreasing: 7.71% --> 7.42%\n",
            "Saving model...\n",
            "39/50 (138m 53s) train loss: 0.348, train error: 12.19%, val loss: 0.265, val error: 7.97%\n",
            "40/50 (142m 27s) train loss: 0.348, train error: 12.07%, val loss: 0.247, val error: 7.65%\n",
            "41/50 (146m 1s) train loss: 0.343, train error: 11.99%, val loss: 0.234, val error: 7.32%\n",
            "Validation error rate is decreasing: 7.42% --> 7.32%\n",
            "Saving model...\n",
            "42/50 (149m 35s) train loss: 0.342, train error: 11.95%, val loss: 0.244, val error: 7.79%\n",
            "43/50 (153m 10s) train loss: 0.338, train error: 11.87%, val loss: 0.235, val error: 7.45%\n",
            "44/50 (156m 44s) train loss: 0.333, train error: 11.70%, val loss: 0.239, val error: 7.47%\n",
            "45/50 (160m 17s) train loss: 0.322, train error: 11.25%, val loss: 0.249, val error: 7.67%\n",
            "46/50 (163m 51s) train loss: 0.326, train error: 11.32%, val loss: 0.260, val error: 7.80%\n",
            "47/50 (167m 25s) train loss: 0.319, train error: 11.12%, val loss: 0.227, val error: 7.01%\n",
            "Validation error rate is decreasing: 7.32% --> 7.01%\n",
            "Saving model...\n",
            "48/50 (170m 59s) train loss: 0.324, train error: 11.22%, val loss: 0.232, val error: 7.13%\n",
            "49/50 (174m 33s) train loss: 0.313, train error: 10.85%, val loss: 0.257, val error: 7.63%\n",
            "50/50 (178m 7s) train loss: 0.314, train error: 10.97%, val loss: 0.237, val error: 6.99%\n",
            "Validation error rate is decreasing: 7.01% --> 6.99%\n",
            "Saving model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_c65cbbf3-9f31-4013-a067-01aa631aad23\", \"nadamdense_train.pt\", 879)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_1fe8dae8-ac57-4ec6-b6d5-971f7b3d6587\", \"nadamdense_test.pt\", 879)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A46QNhXyPveA",
        "outputId": "4668e7b3-aa88-48b0-950b-7f259d58a651"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Dec 15 08:23:00 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P0    54W / 300W |  15619MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGrkBX5rQKUf",
        "outputId": "69f1c990-88e8-46d9-b5ee-40696d80904e"
      },
      "source": [
        "!lsof /dev/nvidia*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "COMMAND  PID USER   FD   TYPE  DEVICE SIZE/OFF  NODE NAME\n",
            "python3 3224 root  mem    CHR 195,255          17360 /dev/nvidiactl\n",
            "python3 3224 root  mem    CHR   195,0          17359 /dev/nvidia0\n",
            "python3 3224 root  mem    CHR   246,0          17357 /dev/nvidia-uvm\n",
            "python3 3224 root   59u   CHR 195,255      0t0 17360 /dev/nvidiactl\n",
            "python3 3224 root   62u   CHR   246,0      0t0 17357 /dev/nvidia-uvm\n",
            "python3 3224 root   63u   CHR   195,0      0t0 17359 /dev/nvidia0\n",
            "python3 3224 root   64u   CHR   195,0      0t0 17359 /dev/nvidia0\n",
            "python3 3224 root   65u   CHR   195,0      0t0 17359 /dev/nvidia0\n",
            "python3 3224 root   70u   CHR   195,0      0t0 17359 /dev/nvidia0\n",
            "python3 3224 root   71u   CHR   195,0      0t0 17359 /dev/nvidia0\n",
            "python3 3224 root   72u   CHR   195,0      0t0 17359 /dev/nvidia0\n",
            "python3 3224 root   75u   CHR   195,0      0t0 17359 /dev/nvidia0\n",
            "python3 3224 root   76u   CHR   195,0      0t0 17359 /dev/nvidia0\n",
            "python3 3224 root   77u   CHR   195,0      0t0 17359 /dev/nvidia0\n",
            "python3 3224 root   78u   CHR   195,0      0t0 17359 /dev/nvidia0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0V-u5q2QBrU"
      },
      "source": [
        "!kill -9 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvV5iX-HFuyO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}