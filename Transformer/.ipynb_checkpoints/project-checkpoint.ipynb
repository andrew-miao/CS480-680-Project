{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yUo4_yooRC8G",
    "outputId": "d1133bf6-fd83-46ff-9edc-51da6b1bf427"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.11.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "P7NK-CXARKLv"
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vRbXFcYtRMyy",
    "outputId": "6df64467-edbe-4be1-8654-b46750cf8659"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.4)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.2)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.11.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
      "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tbsBtT5HR-y2",
    "outputId": "8967a541-6284-4634-b1f3-19cd50e6605b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: de_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz#egg=de_core_news_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.4)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (50.3.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.18.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.4)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.4)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.11.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('de_core_news_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
      "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
      "You can now load the model via spacy.load('de')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XoizEzGs5xBv",
    "outputId": "bf74e58d-0b19-4e15-b5b6-93081b7ef6c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.18.5)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.7.0+cu101)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.11.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.16.0)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LsgpDkyogrzG",
    "outputId": "9c0b2808-6eb5-4708-9ebf-aa2013d97f57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "Building dataset\n",
      "Building dataloader\n",
      "Saving dataloader\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from torchtext.data import Field\n",
    "from torchtext.datasets import Multi30k, WMT14, IWSLT\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "BOS= '<s>'\n",
    "EOS = '</s>'\n",
    "PAD = '<pad>'\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def buildTensor(dataset, max_seq, src_vocab2num, trg_vocab2num, train=True, dev=False, test=False):\n",
    "    src_data = torch.ones(len(dataset), max_seq + 2)\n",
    "    trg_data = None\n",
    "    if not train:\n",
    "        raw_trg = []\n",
    "    \n",
    "    trg_data = torch.ones(len(dataset), max_seq + 2)\n",
    "\n",
    "    for i, sentence in enumerate(dataset):\n",
    "        src_data[i][0] = src_vocab2num[BOS]\n",
    "        src_data[i][-1] = trg_vocab2num[EOS]\n",
    "        for j in range(1, min(max_seq + 1, len(sentence.src))):\n",
    "            word = sentence.src[j - 1]\n",
    "            src_data[i][j] = src_vocab2num[word]\n",
    "\n",
    "        if trg_data is not None:\n",
    "            trg_data[i][0] = trg_vocab2num[BOS]\n",
    "            trg_data[i][-1] = trg_vocab2num[EOS]\n",
    "            for j in range(1, min(max_seq + 1, len(sentence.trg))):\n",
    "                word = sentence.trg[j - 1]\n",
    "                trg_data[i][j] = trg_vocab2num[word]\n",
    "\n",
    "            if not train:\n",
    "                raw_trg.append(sentence.trg)\n",
    "\n",
    "    if train:\n",
    "        return src_data, trg_data\n",
    "\n",
    "    elif dev:\n",
    "        return src_data, trg_data, raw_trg\n",
    "\n",
    "    else:\n",
    "        return src_data, raw_trg\n",
    "\n",
    "print('Loading dataset')\n",
    "\n",
    "SRC = Field(tokenize=tokenize_en, pad_token=PAD)\n",
    "TGT = Field(tokenize=tokenize_de, init_token=BOS, eos_token = EOS, pad_token=PAD)\n",
    "max_seq = 50\n",
    "train, dev, test = Multi30k.splits(exts=('.en', '.de'), fields=(SRC, TGT), filter_pred=lambda x: len(vars(x)['src']) <= max_seq and len(vars(x)['trg']) <= max_seq)\n",
    "\n",
    "min_freq = 2\n",
    "batch_size = 100\n",
    "SRC.build_vocab(train.src, min_freq=min_freq)\n",
    "TGT.build_vocab(train.trg, min_freq=min_freq)\n",
    "\n",
    "src_vocab = SRC.vocab\n",
    "trg_vocab = TGT.vocab\n",
    "\n",
    "src_train_data, trg_train_data = buildTensor(train, max_seq, src_vocab.stoi, trg_vocab.stoi, train=True)\n",
    "src_dev_data, trg_dev_data, raw_dev_trg = buildTensor(dev, max_seq, src_vocab.stoi, trg_vocab.stoi, train=False, dev=True)\n",
    "src_test_data, raw_test_trg = buildTensor(test, max_seq, src_vocab.stoi, trg_vocab.stoi, train=False, test=True)\n",
    "\n",
    "src_train_data, trg_train_data = src_train_data.type(torch.long), trg_train_data.type(torch.long)\n",
    "src_dev_data, trg_dev_data = src_dev_data.type(torch.long), trg_dev_data.type(torch.long)\n",
    "src_test_data = src_test_data.type(torch.long)\n",
    "\n",
    "print('Building dataset')\n",
    "train = TensorDataset(src_train_data, trg_train_data)\n",
    "dev = TensorDataset(src_dev_data, trg_dev_data)\n",
    "test = TensorDataset(src_test_data)\n",
    "\n",
    "print('Building dataloader')\n",
    "train_loader = DataLoader(train, batch_size=batch_size, pin_memory=True)\n",
    "dev_loader = DataLoader(dev, batch_size=batch_size, pin_memory=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, pin_memory=True)\n",
    "\n",
    "print('Saving dataloader')\n",
    "torch.save(src_vocab.stoi, 'src_vocab2num.pt')\n",
    "torch.save(src_vocab.itos, 'src_num2vocab.pt')\n",
    "torch.save(trg_vocab.stoi, 'trg_vocab2num.pt')\n",
    "torch.save(trg_vocab.itos, 'trg_num2vocab.pt')\n",
    "torch.save(raw_dev_trg, 'raw_dev_trg.pt')\n",
    "torch.save(raw_test_trg, 'raw_test_trg.pt')\n",
    "torch.save(train_loader, 'train_loader.pt')\n",
    "torch.save(dev_loader, 'dev_loader.pt')\n",
    "torch.save(test_loader, 'test_loader.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "S0ooXN2J7YTF"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ln4O5JTr2Gkd"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, n_src_vocab, n_trg_vocab, d_model=512, n_heads=8, n_encoders=6, n_decoders=6, d_ff=2048, dropout=0.1, padding_idx=1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.src_embedding = nn.Embedding(n_src_vocab, d_model, padding_idx=padding_idx)\n",
    "        self.trg_embedding = nn.Embedding(n_trg_vocab, d_model, padding_idx=padding_idx)\n",
    "        self.transformer = Transformer(d_model, n_heads, num_encoder_layers=n_encoders, num_decoder_layers=n_decoders,\n",
    "                                       dim_feedforward=d_ff, dropout=dropout)\n",
    "        self.fc = nn.Linear(d_model, n_trg_vocab)\n",
    "\n",
    "    def no_peek_mask(self, size):\n",
    "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask.to(device)\n",
    "    \n",
    "    def forward(self, src_seq, trg_seq):\n",
    "        trg_mask = self.no_peek_mask(trg_seq.size(1))\n",
    "        src_seq, trg_seq = self.src_embedding(src_seq), self.trg_embedding(trg_seq)\n",
    "        src_seq, trg_seq = src_seq.permute(1, 0, 2), trg_seq.permute(1, 0, 2)  # size = (S, B, E), where S = max_seq, B = batch_size, E = embedding_size.\n",
    "        output = self.transformer(src_seq, trg_seq, tgt_mask=trg_mask)\n",
    "        return self.fc(output).permute(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FcLYUn5M2g4_"
   },
   "outputs": [],
   "source": [
    "class TransformerOptim():\n",
    "    def __init__(self, optimizer, d_model=512, warmup_steps=4000):\n",
    "        \"\"\"\n",
    "        :param optimizer: the optimizer that we used, in the original paper, Vaswani et al. use Adam.\n",
    "        :param d_model: the embedding dimension.\n",
    "        :param warmup_steps: the number of warm up training steps.\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.n_steps = 0\n",
    "\n",
    "    def update_lr(self):\n",
    "        \"\"\"\n",
    "        Updated learning rate. lr := d_model^(-0.5) * min(step_num^(-0.5), step_num * warmup_steps^(-1.5))\n",
    "        \"\"\"\n",
    "        self.n_steps += 1\n",
    "        lr = self.d_model**(-0.5) * min(self.n_steps**(-0.5), self.n_steps * self.warmup_steps**(-1.5))\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        self.update_lr()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDYFUO1shoEQ",
    "outputId": "0f083b73-e9f0-4b42-9741-05279a050bc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "Start training\n",
      "1/30, (1m4s), train loss: 2.805, val loss: 1.438\n",
      "Validation loss in first epoch is: 1.4381\n",
      "Saving model\n",
      "2/30, (2m9s), train loss: 1.260, val loss: 1.068\n",
      "Validation loss decreaseing: 1.4381 --> 1.0677\n",
      "Saving model\n",
      "3/30, (3m15s), train loss: 1.006, val loss: 0.900\n",
      "Validation loss decreaseing: 1.0677 --> 0.9001\n",
      "Saving model\n",
      "4/30, (4m21s), train loss: 0.860, val loss: 0.818\n",
      "Validation loss decreaseing: 0.9001 --> 0.8184\n",
      "Saving model\n",
      "5/30, (5m27s), train loss: 0.756, val loss: 0.788\n",
      "Validation loss decreaseing: 0.8184 --> 0.7875\n",
      "Saving model\n",
      "6/30, (6m32s), train loss: 0.681, val loss: 0.769\n",
      "Validation loss decreaseing: 0.7875 --> 0.7686\n",
      "Saving model\n",
      "7/30, (7m38s), train loss: 0.619, val loss: 0.704\n",
      "Validation loss decreaseing: 0.7686 --> 0.7035\n",
      "Saving model\n",
      "8/30, (8m43s), train loss: 0.572, val loss: 0.706\n",
      "9/30, (9m48s), train loss: 0.529, val loss: 0.685\n",
      "Validation loss decreaseing: 0.7035 --> 0.6847\n",
      "Saving model\n",
      "10/30, (10m54s), train loss: 0.489, val loss: 0.660\n",
      "Validation loss decreaseing: 0.6847 --> 0.6602\n",
      "Saving model\n",
      "11/30, (11m59s), train loss: 0.461, val loss: 0.727\n",
      "12/30, (13m4s), train loss: 0.434, val loss: 0.719\n",
      "13/30, (14m9s), train loss: 0.416, val loss: 0.667\n",
      "14/30, (15m14s), train loss: 0.415, val loss: 0.731\n",
      "15/30, (16m19s), train loss: 0.413, val loss: 0.811\n",
      "16/30, (17m24s), train loss: 0.401, val loss: 0.731\n",
      "17/30, (18m28s), train loss: 0.392, val loss: 0.719\n",
      "18/30, (19m33s), train loss: 0.386, val loss: 0.796\n",
      "19/30, (20m38s), train loss: 0.364, val loss: 0.846\n",
      "20/30, (21m42s), train loss: 0.346, val loss: 0.901\n",
      "21/30, (22m47s), train loss: 0.341, val loss: 1.009\n",
      "22/30, (23m52s), train loss: 0.338, val loss: 0.883\n",
      "23/30, (24m57s), train loss: 0.336, val loss: 0.972\n",
      "24/30, (26m1s), train loss: 0.322, val loss: 1.084\n",
      "25/30, (27m6s), train loss: 0.320, val loss: 1.164\n",
      "26/30, (28m11s), train loss: 0.324, val loss: 1.151\n",
      "27/30, (29m15s), train loss: 0.295, val loss: 0.988\n",
      "28/30, (30m20s), train loss: 0.271, val loss: 1.121\n",
      "29/30, (31m25s), train loss: 0.263, val loss: 1.215\n",
      "30/30, (32m29s), train loss: 0.273, val loss: 1.310\n",
      "Training took 32m29s\n",
      "Start testing\n",
      "Saving experiment result\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Author: Yanting Miao\n",
    "\"\"\"\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def embedding(x, n_vocab, device, d_model=512):\n",
    "    embed = nn.Embedding(n_vocab, d_model, padding_idx=0).to(device)\n",
    "    return embed(x).permute(1, 0, 2)\n",
    "\n",
    "def calculate_time(start):\n",
    "    end = time.time()\n",
    "    t = end - start\n",
    "    m = t // 60\n",
    "    s = t - m * 60\n",
    "    return m, s\n",
    "\n",
    "def evaluating(model, data, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for src, trg in data:\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            trg_input = trg[:, :-1]\n",
    "            trg_real = trg[:, 1:]\n",
    "            translate = model(src, trg_input)\n",
    "            loss = criterion(translate, trg_real)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data)\n",
    "\n",
    "def training(model, train_data, dev_data, n_epochs, criterion, optimizer, device, path):\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    model.train()\n",
    "    step = 1\n",
    "    print_every = len(train_data)\n",
    "    min_loss = None\n",
    "    start = time.time()\n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0.0\n",
    "        for src, trg in train_data:\n",
    "            optimizer.zero_grad()\n",
    "            src = src.to(device)\n",
    "            # shifted to right, for example, trg = \"<s>I love cats</s>\", trg_input = \"<s>I love cats\", trg_real = \"I love cats</s>\"\n",
    "            trg_input = trg[:, :-1].to(device)\n",
    "            trg_real = trg[:, 1:].to(device)\n",
    "            translate = model(src, trg_input)\n",
    "            loss = criterion(translate, trg_real)\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            step += 1\n",
    "            if step % print_every == 0:\n",
    "                val_loss = evaluating(model, dev_data, criterion, device)\n",
    "                m, s = calculate_time(start)\n",
    "                train_loss_list.append(running_loss / len(train_data))\n",
    "                val_loss_list.append(val_loss)\n",
    "                print('%d/%d, (%dm%ds), train loss: %.3f, val loss: %.3f' %\n",
    "                      (epoch + 1, n_epochs, m, s, running_loss / len(train_data), val_loss))\n",
    "                if min_loss is None or min_loss > val_loss:\n",
    "                    if min_loss:\n",
    "                        print('Validation loss decreaseing: %.4f --> %.4f' % (min_loss, val_loss))\n",
    "                    else:\n",
    "                        print('Validation loss in first epoch is: %.4f' % (val_loss))\n",
    "                    print('Saving model')\n",
    "                    min_loss = val_loss\n",
    "                    torch.save(model, path)\n",
    "                running_loss = 0.0\n",
    "                model.train()\n",
    "    return train_loss_list, val_loss_list\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n_epochs = 30\n",
    "    max_seq = 100\n",
    "    optim_name = 'Adam'\n",
    "    print('Loading dataset')\n",
    "    train_data = torch.load('train_loader.pt')\n",
    "    dev_data = torch.load('dev_loader.pt')\n",
    "    test_data = torch.load('test_loader.pt')\n",
    "    src_vocab2num = torch.load('src_vocab2num.pt')\n",
    "    trg_vocab2num = torch.load('trg_vocab2num.pt')\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = TransformerModel(len(src_vocab2num), len(trg_vocab2num), 512, 1, n_encoders=6, n_decoders=6, d_ff=2048).to(device)\n",
    "    path = 'best_adam_transformer.pt'\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    adam_optim = optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9)\n",
    "    optimizer = TransformerOptim(adam_optim)\n",
    "    print('Start training')\n",
    "    start = time.time()\n",
    "    train_loss, val_loss = training(model, train_data, dev_data, n_epochs, criterion, optimizer, device, path)\n",
    "    m, s = calculate_time(start)\n",
    "    print('Training took %dm%ds' % (m, s))\n",
    "    print('Start testing')\n",
    "    model = torch.load(path)\n",
    "    model = model.to(device)\n",
    "    # test_loss = evaluating(model, test_data, criterion, device)\n",
    "    # print('Test loss: %.3f' % (test_loss))\n",
    "    print('Saving experiment result')\n",
    "    train_loss_path = optim_name + '_train_loss.pt'\n",
    "    val_loss_path = optim_name + '_val_loss.pt'\n",
    "    # test_loss_path = optim_name + '_test_loss.pt'\n",
    "    torch.save(train_loss, train_loss_path)\n",
    "    torch.save(val_loss, val_loss_path)\n",
    "    # torch.save(test_loss, test_loss_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "l1hUw3mVb_QS"
   },
   "outputs": [],
   "source": [
    "def translate(model, sentence, trg_num2vocab, device):\n",
    "    model.eval()\n",
    "    sentence = tokenize_en(sentence[0])\n",
    "    tmp = []\n",
    "    for token in sentence:\n",
    "        if token in src_vocab2num:\n",
    "            tmp.append(src_vocab2num[token])\n",
    "        else:\n",
    "            tmp.append(src_vocab2num['<unk>'])\n",
    "    sentence = torch.LongTensor([tmp]).to(device)\n",
    "    sentence = sentence.view(1, -1)\n",
    "    trg_init_tok = trg_vocab2num[BOS]\n",
    "    trg = torch.LongTensor([[trg_init_tok]]).to(device)\n",
    "    translation = \"\"\n",
    "    for i in range(max_seq):\n",
    "        pred = model(sentence, trg)\n",
    "        pred = torch.argmax(pred, dim=1)\n",
    "        new_word = pred[0][i].item()\n",
    "        add_word = trg_num2vocab[new_word]\n",
    "        if add_word == EOS or add_word == PAD:\n",
    "            break\n",
    "        translation += \" \" + add_word\n",
    "        pred = torch.LongTensor([[new_word]]).to(device)\n",
    "        trg = torch.cat((trg, pred), dim=1).to(device)\n",
    "    \n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HoEhAqFRL2ua"
   },
   "outputs": [],
   "source": [
    "def reconstructTrgLine(raw_data, num2vocab):\n",
    "    sentences = []\n",
    "    for line in raw_data:\n",
    "        tmp = ''\n",
    "        for token in line:\n",
    "            if token != BOS and token != EOS and token != PAD:\n",
    "                token = token + ' '\n",
    "                tmp += token\n",
    "        sentences.append(tmp)\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Pdy3vpP-jdkS"
   },
   "outputs": [],
   "source": [
    "def reconstructSrcLine(data, num2vocab):\n",
    "    sentences = []\n",
    "    for line in data:\n",
    "        tmp = ''\n",
    "        for token in line:\n",
    "            word = num2vocab[token.item()]\n",
    "            if word != BOS and word != EOS and word != PAD and word != '<unk>':\n",
    "                word += ' '\n",
    "                tmp += word\n",
    "        sentences.append([tmp])\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7RQfvlHyh-mK"
   },
   "outputs": [],
   "source": [
    "raw_test_trg_sentences = reconstructTrgLine(raw_test_trg, TGT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Si-lAM15iU5-"
   },
   "outputs": [],
   "source": [
    "raw_test_src_sentences = reconstructSrcLine(src_test_data, SRC.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TT5-D6xok43K",
    "outputId": "6f60dd43-51de-4d11-cea8-63b5b4160c04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (1.18.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (4.41.1)\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ifmz17kTlssi"
   },
   "outputs": [],
   "source": [
    "from torchnlp.metrics import get_moses_multi_bleu\n",
    "\n",
    "model = torch.load('best_adam_transformer.pt')\n",
    "translation = [None] * len(raw_test_src_sentences)\n",
    "for i in range(len(translation)):\n",
    "    translation[i] = translate(model, raw_test_src_sentences[i], TGT.vocab.itos, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "juQhTmjvnIl2"
   },
   "outputs": [],
   "source": [
    "bleu = get_moses_multi_bleu(translation, raw_test_trg_sentences, lowercase=True)\n",
    "torch.save(bleu, 'adam_bleu.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S6ngBNEqK-pr",
    "outputId": "6ec7933e-cb2e-4805-e603-f774ddfd53ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.94"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "RPwec9ksOIN9",
    "outputId": "64976c11-29fe-4574-f4ad-d98e90973b35"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Ein Mann in einem schwarzen Oberteil und mit der Nummer \\xa0 5 an der Seite \\xa0 „ \\xa0 5 “ ist im Freien zu sehen'"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, 'hello world', TGT.vocab.itos, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "-Oaavaqb2Hcj",
    "outputId": "d059f661-3e37-4550-baae-249994b34c93"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_b1431a11-7b6f-460a-95b2-7d37d5ba3ab2\", \"adam_bleu.pt\", 559)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download('adam_bleu.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kI5ADPdH3N_b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
